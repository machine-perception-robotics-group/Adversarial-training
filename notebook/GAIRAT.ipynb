{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be933234-4b51-41a5-a5a0-9b831b916f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Geometry-Awere Instance-Reweighted Adversarial Training (GAIRAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85eb3db2-6a80-4e98-8a62-4fd714fc9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import yaml\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66240c81-1be4-4a34-af22-21a6292e0ff9",
   "metadata": {},
   "source": [
    "## Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01de849e-a804-4062-aef7-4cfcd5999b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '4'\n",
    "dataset = 'cifar10'\n",
    "model_type = 'wrn34-10'\n",
    "checkpoint = './checkpoint/%s/%s' % (model_type, dataset)\n",
    "num_classes = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0035\n",
    "batch_size = 128\n",
    "total_epochs = 100\n",
    "lam = -1\n",
    "epsilon = 8/255\n",
    "alpha = 2/255\n",
    "num_repeats = 10\n",
    "warm_up = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c102405-4d46-45e0-b957-9e7022b7ae2f",
   "metadata": {},
   "source": [
    "## Inner maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee44a0f-0acc-41c3-95a0-e477e167e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats):\n",
    "    noise = torch.FloatTensor(inputs.shape).uniform_(-epsilon, epsilon).cuda()\n",
    "    x = torch.clamp(inputs + noise, min=0, max=1)\n",
    "    kappa = torch.zeros(inputs.size(0)).cuda()\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        x.requires_grad_()\n",
    "        logits = model(x)\n",
    "        kappa += logits.softmax(dim=1).argmax(dim=1).eq(targets)\n",
    "        loss = xent(logits, targets)\n",
    "        loss.backward()\n",
    "        grads = x.grad.data\n",
    "        x = x.detach() + alpha*torch.sign(grads).detach()\n",
    "        x = torch.min(torch.max(x, inputs-epsilon), inputs+epsilon).clamp(min=0, max=1)\n",
    "    return x, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2591b-4ebf-4714-a3f4-85cdac7f272d",
   "metadata": {},
   "source": [
    "## Training (Outer minimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2276c9-d52b-4544-8448-42817161084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, optimizer, num_classes, warm_up, \n",
    "             lam=-1, epsilon=8/255, alpha=2/255, num_repeats=10):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "        \n",
    "    tanh = nn.Tanh()\n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        batch = inputs.size(0)\n",
    "        \n",
    "        x, kappa = pgd(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "        logits = model(x)\n",
    "        s = (1 + tanh(lam + 5*(1 - 2*kappa/num_repeats)))/2\n",
    "        s = s/torch.sum(weights)\n",
    "        \n",
    "        if warm_up < epoch:\n",
    "            class_index = torch.arange(logits.size(1))[None,:].repeat(batch,1).cuda()\n",
    "            loss = -torch.sum(s * torch.log_softmax(logits, dim=1)[class_index==targets[:,None]])/batch\n",
    "        else:\n",
    "            loss = xent(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total += batch\n",
    "        total_loss += loss.item()\n",
    "        num_correct = torch.argmax(logits.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct += num_correct\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Epoch %d [%d/%d] | loss: %.4f (avg: %.4f) | acc: %.4f (avg: %.4f) |'\\\n",
    "                  % (epoch, idx, len(dataloader), loss.item(), total_loss/len(dataloader),\n",
    "                     num_correct/batch, total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1c8d2-8e9c-441c-b471-9eb237eb3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch, model, dataloader, alpha, epsilon, num_repeats):\n",
    "    model.eval()\n",
    "    total_correct_nat = 0\n",
    "    total_correct_adv = 0\n",
    "    \n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for samples in dataloader:\n",
    "        inputs, targets = samples[0].cuda(), samples[1].cuda()\n",
    "        batch = inputs.size(0)\n",
    "        with torch.enable_grad():\n",
    "            x = pgd(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits_nat = model(inputs)\n",
    "            logits_adv = model(x)\n",
    "        \n",
    "        total_correct_nat += torch.argmax(logits_nat.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct_adv += torch.argmax(logits_adv.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        \n",
    "    print('Validation | acc (nat): %.4f | acc (rob): %.4f |' % (total_correct_nat / len(dataloader.dataset),\n",
    "                                                                total_correct_adv / len(dataloader.dataset)))\n",
    "    return (total_correct_nat / len(dataloader.dataset)), (total_correct_adv / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208f000e-cb35-4ad5-93d5-fe47f788b266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (dim=int, ), but expected one of:\n * (Tensor input, name dim, *, torch.dtype dtype)\n * (Tensor input, int dim, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-823656e9b742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     training(epoch, model, train_dataloader, optimizer, num_classes,\n\u001b[0;32m---> 25\u001b[0;31m              lam1, lam2, gamma, epsilon, alpha, num_repeats)\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtest_acc_nat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_rob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6adc23e26481>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, model, dataloader, optimizer, num_classes, lam1, lam2, gamma, epsilon, alpha, num_repeats)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (dim=int, ), but expected one of:\n * (Tensor input, name dim, *, torch.dtype dtype)\n * (Tensor input, int dim, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()])\n",
    "train_dataset, _ = get_dataloader(dataset, batch_size)\n",
    "num_samples = len(train_dataset)\n",
    "num_samples_for_train = int(num_samples * 0.98)\n",
    "num_samples_for_valid = num_samples - num_samples_for_train\n",
    "train_set, valid_set = random_split(train_dataset, [num_samples_for_train, num_samples_for_valid])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "model = nn.DataParallel(get_network(model_type, num_classes).cuda())\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = [int(total_epochs*0.5), int(total_epochs*0.75)]\n",
    "adjust_learning_rate = lr_scheduler.MultiStepLR(optimizer, scheduler, gamma=0.1)\n",
    "best_acc_nat, best_acc_rob = 0, 0\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    training(epoch, model, train_dataloader, optimizer, num_classes, warm_up, lam, epsilon, alpha, num_repeats)\n",
    "    test_acc_nat, test_acc_rob = evaluation(epoch, model, valid_dataloader)\n",
    "        \n",
    "    is_best = best_acc_nat < test_acc_nat and best_acc_rob < test_acc_rob\n",
    "    best_acc_nat = max(best_acc_nat, test_acc_nat)\n",
    "    best_acc_rob = max(best_acc_rob, test_acc_rob)\n",
    "    save_checkpoint = {'state_dict': model.state_dict(),\n",
    "                       'best_acc': best_acc,\n",
    "                       'test_acc': test_acc,\n",
    "                       'optimizer': optimizer.state_dict(),\n",
    "                       'model_type': model_type,\n",
    "                       'dataset': dataset}\n",
    "    torch.save(save_checkpoint, os.path.join(checkpoint, 'model'))\n",
    "    if is_best:\n",
    "        torch.save(save_checkpoint, os.path.join(checkpoint, 'best_model'))\n",
    "    adjust_learning_rate.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67273057-ddb6-4294-b7ae-3aa932ab0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
