{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be933234-4b51-41a5-a5a0-9b831b916f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Misclassification Aware Adversarial Training (MART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85eb3db2-6a80-4e98-8a62-4fd714fc9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import yaml\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66240c81-1be4-4a34-af22-21a6292e0ff9",
   "metadata": {},
   "source": [
    "## Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01de849e-a804-4062-aef7-4cfcd5999b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '0,1,2,3'\n",
    "dataset = 'cifar10'\n",
    "model_type = 'wrn34-10'\n",
    "checkpoint = './checkpoint/mart/%s/%s' % (model_type, dataset)\n",
    "num_classes = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 3.5e-3\n",
    "batch_size = 128\n",
    "total_epochs = 100\n",
    "beta = 5.0\n",
    "epsilon = 8/255\n",
    "alpha = 2/255\n",
    "num_repeats = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c102405-4d46-45e0-b957-9e7022b7ae2f",
   "metadata": {},
   "source": [
    "## Inner maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee44a0f-0acc-41c3-95a0-e477e167e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats):\n",
    "    noise = torch.FloatTensor(inputs.shape).uniform_(-1, 1).cuda()\n",
    "    x = inputs + noise\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        x.requires_grad_()\n",
    "        logits = model(x)\n",
    "        loss = xent(logits, targets)\n",
    "        loss.backward()\n",
    "        grads = x.grad.data\n",
    "        x = x.detach() + alpha*torch.sign(grads).detach()\n",
    "        x = torch.min(torch.max(x, inputs-epsilon), inputs+epsilon).clamp(min=0, max=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2591b-4ebf-4714-a3f4-85cdac7f272d",
   "metadata": {},
   "source": [
    "## Training (Outer minimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2276c9-d52b-4544-8448-42817161084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, optimizer, num_classes, \n",
    "             beta=6.0, epsilon=8/255, alpha=2/255, num_repeats=10):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "        \n",
    "    kl = nn.KLDivLoss(reduction='none')\n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        batch = inputs.size(0)\n",
    "        \n",
    "        x = inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "        logits_adv = model(x)\n",
    "        logits_nat = model(inputs)\n",
    "        \n",
    "        classes = torch.arange(num_classes)[None,:].repeat(batch,1).cuda()\n",
    "        log_softmax_gt = torch.log_softmax(logits_adv, dim=1)[classes==targets[:,None]].unsqueeze(1)\n",
    "        false_probs = torch.log_softmax(logits_adv, dim=1)[classes!=targets[:,None]].view(batch, num_classes-1)\n",
    "        top2_probs = torch.topk(false_probs, k=1, dim=1).values\n",
    "        boosted_xent_loss = torch.sum(-log_softmax_gt - torch.log(1 - top2_probs))/batch\n",
    "        kl_loss = torch.sum(kl(torch.log_softmax(logits_adv, dim=1),\n",
    "                            torch.softmax(logits_nat, dim=1)), dim=1)\n",
    "        loss = boosted_xent_loss + beta*torch.sum(kl_loss*(1-logits_nat.softmax(dim=1)[classes==targets[:,None]]))/batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total += batch\n",
    "        total_loss += loss.item()\n",
    "        num_correct = torch.argmax(logits_adv.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct += num_correct\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Epoch %d [%d/%d] | loss: %.4f (avg: %.4f) | acc: %.4f (avg: %.4f) |'\\\n",
    "                  % (epoch, idx, len(dataloader), loss.item(), total_loss/len(dataloader),\n",
    "                     num_correct/batch, total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1c8d2-8e9c-441c-b471-9eb237eb3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch, model, dataloader, alpha, epsilon, num_repeats):\n",
    "    model.eval()\n",
    "    total_correct_nat = 0\n",
    "    total_correct_adv = 0\n",
    "    \n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for samples in dataloader:\n",
    "        inputs, targets = samples[0].cuda(), samples[1].cuda()\n",
    "        batch = inputs.size(0)\n",
    "        with torch.enable_grad():\n",
    "            x = inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits_nat = model(inputs)\n",
    "            logits_adv = model(x)\n",
    "        \n",
    "        total_correct_nat += torch.argmax(logits_nat.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct_adv += torch.argmax(logits_adv.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        \n",
    "    print('Validation | acc (nat): %.4f | acc (rob): %.4f |' % (total_correct_nat / len(dataloader.dataset),\n",
    "                                                                total_correct_adv / len(dataloader.dataset)))\n",
    "    return (total_correct_nat / len(dataloader.dataset)), (total_correct_adv / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208f000e-cb35-4ad5-93d5-fe47f788b266",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0 [0/383] | loss: 2.1897 (avg: 0.0057) | acc: 0.0000 (avg: 0.0000) |\n",
      "Epoch 0 [100/383] | loss: 1.1616 (avg: 0.3332) | acc: 0.0781 (avg: 0.0726) |\n",
      "Epoch 0 [200/383] | loss: 1.1334 (avg: 0.6308) | acc: 0.1562 (avg: 0.1088) |\n",
      "Epoch 0 [300/383] | loss: 1.0833 (avg: 0.9230) | acc: 0.2109 (avg: 0.1272) |\n",
      "Validation | acc (nat): 0.2200 | acc (rob): 0.1840 |\n",
      "Epoch 1 [0/383] | loss: 1.1087 (avg: 0.0029) | acc: 0.1719 (avg: 0.1719) |\n",
      "Epoch 1 [100/383] | loss: 1.0971 (avg: 0.2894) | acc: 0.1484 (avg: 0.1866) |\n",
      "Epoch 1 [200/383] | loss: 1.0590 (avg: 0.5744) | acc: 0.2422 (avg: 0.1947) |\n",
      "Epoch 1 [300/383] | loss: 1.0661 (avg: 0.8577) | acc: 0.1484 (avg: 0.1975) |\n",
      "Validation | acc (nat): 0.2510 | acc (rob): 0.1960 |\n",
      "Epoch 2 [0/383] | loss: 1.0677 (avg: 0.0028) | acc: 0.2344 (avg: 0.2344) |\n",
      "Epoch 2 [100/383] | loss: 1.0950 (avg: 0.2844) | acc: 0.2344 (avg: 0.2251) |\n",
      "Epoch 2 [200/383] | loss: 1.0581 (avg: 0.5645) | acc: 0.1719 (avg: 0.2288) |\n",
      "Epoch 2 [300/383] | loss: 1.1008 (avg: 0.8422) | acc: 0.1875 (avg: 0.2314) |\n",
      "Validation | acc (nat): 0.3010 | acc (rob): 0.2270 |\n",
      "Epoch 3 [0/383] | loss: 1.0321 (avg: 0.0027) | acc: 0.1719 (avg: 0.1719) |\n",
      "Epoch 3 [100/383] | loss: 1.0796 (avg: 0.2794) | acc: 0.2344 (avg: 0.2370) |\n",
      "Epoch 3 [200/383] | loss: 1.0727 (avg: 0.5530) | acc: 0.2266 (avg: 0.2462) |\n",
      "Epoch 3 [300/383] | loss: 1.0154 (avg: 0.8282) | acc: 0.3359 (avg: 0.2458) |\n",
      "Validation | acc (nat): 0.3280 | acc (rob): 0.2640 |\n",
      "Epoch 4 [0/383] | loss: 1.0247 (avg: 0.0027) | acc: 0.3047 (avg: 0.3047) |\n",
      "Epoch 4 [100/383] | loss: 1.0212 (avg: 0.2762) | acc: 0.2891 (avg: 0.2577) |\n",
      "Epoch 4 [200/383] | loss: 1.0112 (avg: 0.5457) | acc: 0.2188 (avg: 0.2618) |\n",
      "Epoch 4 [300/383] | loss: 1.0817 (avg: 0.8156) | acc: 0.2266 (avg: 0.2628) |\n",
      "Validation | acc (nat): 0.3560 | acc (rob): 0.2710 |\n",
      "Epoch 5 [0/383] | loss: 0.9822 (avg: 0.0026) | acc: 0.3047 (avg: 0.3047) |\n",
      "Epoch 5 [100/383] | loss: 1.0864 (avg: 0.2695) | acc: 0.2812 (avg: 0.2758) |\n",
      "Epoch 5 [200/383] | loss: 1.0010 (avg: 0.5359) | acc: 0.3047 (avg: 0.2783) |\n",
      "Epoch 5 [300/383] | loss: 1.0414 (avg: 0.7999) | acc: 0.2188 (avg: 0.2825) |\n",
      "Validation | acc (nat): 0.3780 | acc (rob): 0.2850 |\n",
      "Epoch 6 [0/383] | loss: 1.0254 (avg: 0.0027) | acc: 0.2969 (avg: 0.2969) |\n",
      "Epoch 6 [100/383] | loss: 0.9730 (avg: 0.2630) | acc: 0.3047 (avg: 0.2994) |\n",
      "Epoch 6 [200/383] | loss: 0.9866 (avg: 0.5257) | acc: 0.3359 (avg: 0.2970) |\n",
      "Epoch 6 [300/383] | loss: 1.0416 (avg: 0.7844) | acc: 0.2500 (avg: 0.3014) |\n",
      "Validation | acc (nat): 0.4120 | acc (rob): 0.2920 |\n",
      "Epoch 7 [0/383] | loss: 0.9586 (avg: 0.0025) | acc: 0.3750 (avg: 0.3750) |\n",
      "Epoch 7 [100/383] | loss: 0.9648 (avg: 0.2578) | acc: 0.2812 (avg: 0.3104) |\n",
      "Epoch 7 [200/383] | loss: 0.9626 (avg: 0.5147) | acc: 0.3359 (avg: 0.3128) |\n",
      "Epoch 7 [300/383] | loss: 1.0275 (avg: 0.7690) | acc: 0.3125 (avg: 0.3144) |\n",
      "Validation | acc (nat): 0.4600 | acc (rob): 0.3510 |\n",
      "Epoch 8 [0/383] | loss: 1.0085 (avg: 0.0026) | acc: 0.2656 (avg: 0.2656) |\n",
      "Epoch 8 [100/383] | loss: 0.9612 (avg: 0.2527) | acc: 0.3438 (avg: 0.3362) |\n",
      "Epoch 8 [200/383] | loss: 0.9286 (avg: 0.5030) | acc: 0.3594 (avg: 0.3370) |\n",
      "Epoch 8 [300/383] | loss: 0.9907 (avg: 0.7522) | acc: 0.3125 (avg: 0.3369) |\n",
      "Validation | acc (nat): 0.4740 | acc (rob): 0.3480 |\n",
      "Epoch 9 [0/383] | loss: 0.9495 (avg: 0.0025) | acc: 0.3203 (avg: 0.3203) |\n",
      "Epoch 9 [100/383] | loss: 0.9060 (avg: 0.2486) | acc: 0.4531 (avg: 0.3406) |\n",
      "Epoch 9 [200/383] | loss: 0.9383 (avg: 0.4935) | acc: 0.3438 (avg: 0.3434) |\n",
      "Epoch 9 [300/383] | loss: 0.9279 (avg: 0.7379) | acc: 0.3125 (avg: 0.3457) |\n",
      "Validation | acc (nat): 0.4900 | acc (rob): 0.3640 |\n",
      "Epoch 10 [0/383] | loss: 0.9203 (avg: 0.0024) | acc: 0.3047 (avg: 0.3047) |\n",
      "Epoch 10 [100/383] | loss: 0.9723 (avg: 0.2434) | acc: 0.2812 (avg: 0.3598) |\n",
      "Epoch 10 [200/383] | loss: 0.8565 (avg: 0.4811) | acc: 0.4219 (avg: 0.3617) |\n",
      "Epoch 10 [300/383] | loss: 0.9524 (avg: 0.7189) | acc: 0.3750 (avg: 0.3636) |\n",
      "Validation | acc (nat): 0.5120 | acc (rob): 0.3690 |\n",
      "Epoch 11 [0/383] | loss: 0.9230 (avg: 0.0024) | acc: 0.3672 (avg: 0.3672) |\n",
      "Epoch 11 [100/383] | loss: 0.8304 (avg: 0.2380) | acc: 0.4062 (avg: 0.3695) |\n",
      "Epoch 11 [200/383] | loss: 0.8661 (avg: 0.4706) | acc: 0.3828 (avg: 0.3770) |\n",
      "Epoch 11 [300/383] | loss: 0.9246 (avg: 0.7028) | acc: 0.3516 (avg: 0.3763) |\n",
      "Validation | acc (nat): 0.5760 | acc (rob): 0.3850 |\n",
      "Epoch 12 [0/383] | loss: 0.8086 (avg: 0.0021) | acc: 0.3750 (avg: 0.3750) |\n",
      "Epoch 12 [100/383] | loss: 0.8430 (avg: 0.2301) | acc: 0.4219 (avg: 0.3853) |\n",
      "Epoch 12 [200/383] | loss: 0.8332 (avg: 0.4584) | acc: 0.4062 (avg: 0.3858) |\n",
      "Epoch 12 [300/383] | loss: 0.9066 (avg: 0.6849) | acc: 0.3984 (avg: 0.3898) |\n",
      "Validation | acc (nat): 0.5410 | acc (rob): 0.3910 |\n",
      "Epoch 13 [0/383] | loss: 0.8401 (avg: 0.0022) | acc: 0.3984 (avg: 0.3984) |\n",
      "Epoch 13 [100/383] | loss: 0.8701 (avg: 0.2260) | acc: 0.3906 (avg: 0.3950) |\n",
      "Epoch 13 [200/383] | loss: 0.8730 (avg: 0.4491) | acc: 0.4219 (avg: 0.3957) |\n",
      "Epoch 13 [300/383] | loss: 0.8736 (avg: 0.6700) | acc: 0.3906 (avg: 0.3978) |\n",
      "Validation | acc (nat): 0.5630 | acc (rob): 0.3980 |\n",
      "Epoch 14 [0/383] | loss: 0.8950 (avg: 0.0023) | acc: 0.3750 (avg: 0.3750) |\n",
      "Epoch 14 [100/383] | loss: 0.8287 (avg: 0.2193) | acc: 0.4062 (avg: 0.4057) |\n",
      "Epoch 14 [200/383] | loss: 0.8388 (avg: 0.4388) | acc: 0.3750 (avg: 0.4050) |\n",
      "Epoch 14 [300/383] | loss: 0.7595 (avg: 0.6558) | acc: 0.4297 (avg: 0.4060) |\n",
      "Validation | acc (nat): 0.5630 | acc (rob): 0.4150 |\n",
      "Epoch 15 [0/383] | loss: 0.8418 (avg: 0.0022) | acc: 0.4297 (avg: 0.4297) |\n",
      "Epoch 15 [100/383] | loss: 0.7429 (avg: 0.2133) | acc: 0.4766 (avg: 0.4185) |\n",
      "Epoch 15 [200/383] | loss: 0.7865 (avg: 0.4262) | acc: 0.4922 (avg: 0.4169) |\n",
      "Epoch 15 [300/383] | loss: 0.7871 (avg: 0.6403) | acc: 0.4297 (avg: 0.4152) |\n",
      "Validation | acc (nat): 0.5680 | acc (rob): 0.4280 |\n",
      "Epoch 16 [0/383] | loss: 0.8345 (avg: 0.0022) | acc: 0.3828 (avg: 0.3828) |\n",
      "Epoch 16 [100/383] | loss: 0.7145 (avg: 0.2095) | acc: 0.5000 (avg: 0.4221) |\n",
      "Epoch 16 [200/383] | loss: 0.8171 (avg: 0.4180) | acc: 0.3828 (avg: 0.4250) |\n",
      "Epoch 16 [300/383] | loss: 0.8618 (avg: 0.6253) | acc: 0.4219 (avg: 0.4244) |\n",
      "Validation | acc (nat): 0.6030 | acc (rob): 0.4360 |\n",
      "Epoch 17 [0/383] | loss: 0.8958 (avg: 0.0023) | acc: 0.4219 (avg: 0.4219) |\n",
      "Epoch 17 [100/383] | loss: 0.7618 (avg: 0.2063) | acc: 0.4531 (avg: 0.4263) |\n",
      "Epoch 17 [200/383] | loss: 0.7653 (avg: 0.4113) | acc: 0.4219 (avg: 0.4258) |\n",
      "Epoch 17 [300/383] | loss: 0.7526 (avg: 0.6146) | acc: 0.4922 (avg: 0.4259) |\n",
      "Validation | acc (nat): 0.6150 | acc (rob): 0.4500 |\n",
      "Epoch 18 [0/383] | loss: 0.7501 (avg: 0.0020) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 18 [100/383] | loss: 0.8011 (avg: 0.2001) | acc: 0.4766 (avg: 0.4394) |\n",
      "Epoch 18 [200/383] | loss: 0.9668 (avg: 0.3989) | acc: 0.3359 (avg: 0.4382) |\n",
      "Epoch 18 [300/383] | loss: 0.7322 (avg: 0.5989) | acc: 0.4453 (avg: 0.4363) |\n",
      "Validation | acc (nat): 0.6110 | acc (rob): 0.4410 |\n",
      "Epoch 19 [0/383] | loss: 0.6550 (avg: 0.0017) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 19 [100/383] | loss: 0.6419 (avg: 0.1983) | acc: 0.4766 (avg: 0.4409) |\n",
      "Epoch 19 [200/383] | loss: 0.7798 (avg: 0.3932) | acc: 0.4688 (avg: 0.4417) |\n",
      "Epoch 19 [300/383] | loss: 0.6295 (avg: 0.5896) | acc: 0.5312 (avg: 0.4428) |\n",
      "Validation | acc (nat): 0.6110 | acc (rob): 0.4620 |\n",
      "Epoch 20 [0/383] | loss: 0.8114 (avg: 0.0021) | acc: 0.4375 (avg: 0.4375) |\n",
      "Epoch 20 [100/383] | loss: 0.8706 (avg: 0.1887) | acc: 0.3984 (avg: 0.4591) |\n",
      "Epoch 20 [200/383] | loss: 0.7011 (avg: 0.3867) | acc: 0.4219 (avg: 0.4480) |\n",
      "Epoch 20 [300/383] | loss: 0.8154 (avg: 0.5798) | acc: 0.3984 (avg: 0.4474) |\n",
      "Validation | acc (nat): 0.6470 | acc (rob): 0.4480 |\n",
      "Epoch 21 [0/383] | loss: 0.8107 (avg: 0.0021) | acc: 0.3516 (avg: 0.3516) |\n",
      "Epoch 21 [100/383] | loss: 0.6978 (avg: 0.1901) | acc: 0.3984 (avg: 0.4476) |\n",
      "Epoch 21 [200/383] | loss: 0.5900 (avg: 0.3776) | acc: 0.5000 (avg: 0.4505) |\n",
      "Epoch 21 [300/383] | loss: 0.7529 (avg: 0.5680) | acc: 0.4531 (avg: 0.4489) |\n",
      "Validation | acc (nat): 0.6220 | acc (rob): 0.4600 |\n",
      "Epoch 22 [0/383] | loss: 0.7138 (avg: 0.0019) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 22 [100/383] | loss: 0.7072 (avg: 0.1858) | acc: 0.4531 (avg: 0.4522) |\n",
      "Epoch 22 [200/383] | loss: 0.6865 (avg: 0.3723) | acc: 0.4531 (avg: 0.4532) |\n",
      "Epoch 22 [300/383] | loss: 0.6859 (avg: 0.5592) | acc: 0.4531 (avg: 0.4548) |\n",
      "Validation | acc (nat): 0.6320 | acc (rob): 0.4410 |\n",
      "Epoch 23 [0/383] | loss: 0.6811 (avg: 0.0018) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 23 [100/383] | loss: 0.6427 (avg: 0.1846) | acc: 0.4609 (avg: 0.4599) |\n",
      "Epoch 23 [200/383] | loss: 0.6974 (avg: 0.3704) | acc: 0.4609 (avg: 0.4544) |\n",
      "Epoch 23 [300/383] | loss: 0.8400 (avg: 0.5571) | acc: 0.4531 (avg: 0.4536) |\n",
      "Validation | acc (nat): 0.6590 | acc (rob): 0.4570 |\n",
      "Epoch 24 [0/383] | loss: 0.6881 (avg: 0.0018) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 24 [100/383] | loss: 0.7378 (avg: 0.1789) | acc: 0.4609 (avg: 0.4659) |\n",
      "Epoch 24 [200/383] | loss: 0.7853 (avg: 0.3649) | acc: 0.4453 (avg: 0.4599) |\n",
      "Epoch 24 [300/383] | loss: 0.6709 (avg: 0.5416) | acc: 0.4531 (avg: 0.4597) |\n",
      "Validation | acc (nat): 0.6250 | acc (rob): 0.4330 |\n",
      "Epoch 25 [0/383] | loss: 0.6102 (avg: 0.0016) | acc: 0.4922 (avg: 0.4922) |\n",
      "Epoch 25 [100/383] | loss: 0.7840 (avg: 0.1778) | acc: 0.4375 (avg: 0.4676) |\n",
      "Epoch 25 [200/383] | loss: 0.6082 (avg: 0.3575) | acc: 0.4688 (avg: 0.4629) |\n",
      "Epoch 25 [300/383] | loss: 0.5729 (avg: 0.5394) | acc: 0.5078 (avg: 0.4613) |\n",
      "Validation | acc (nat): 0.6220 | acc (rob): 0.4370 |\n",
      "Epoch 26 [0/383] | loss: 0.6802 (avg: 0.0018) | acc: 0.5000 (avg: 0.5000) |\n",
      "Epoch 26 [100/383] | loss: 0.5965 (avg: 0.1760) | acc: 0.4922 (avg: 0.4682) |\n",
      "Epoch 26 [200/383] | loss: 0.6108 (avg: 0.3540) | acc: 0.4766 (avg: 0.4639) |\n",
      "Epoch 26 [300/383] | loss: 0.6919 (avg: 0.5345) | acc: 0.4688 (avg: 0.4625) |\n",
      "Validation | acc (nat): 0.6170 | acc (rob): 0.4440 |\n",
      "Epoch 27 [0/383] | loss: 0.5155 (avg: 0.0013) | acc: 0.5625 (avg: 0.5625) |\n",
      "Epoch 27 [100/383] | loss: 0.6684 (avg: 0.1754) | acc: 0.4844 (avg: 0.4640) |\n",
      "Epoch 27 [200/383] | loss: 0.7097 (avg: 0.3510) | acc: 0.4219 (avg: 0.4650) |\n",
      "Epoch 27 [300/383] | loss: 0.9962 (avg: 0.5268) | acc: 0.3828 (avg: 0.4661) |\n",
      "Validation | acc (nat): 0.6650 | acc (rob): 0.4670 |\n",
      "Epoch 28 [0/383] | loss: 0.6328 (avg: 0.0017) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 28 [100/383] | loss: 0.3891 (avg: 0.1751) | acc: 0.5391 (avg: 0.4742) |\n",
      "Epoch 28 [200/383] | loss: 0.7185 (avg: 0.3496) | acc: 0.4375 (avg: 0.4726) |\n",
      "Epoch 28 [300/383] | loss: 0.5681 (avg: 0.5218) | acc: 0.4922 (avg: 0.4688) |\n",
      "Validation | acc (nat): 0.6300 | acc (rob): 0.4350 |\n",
      "Epoch 29 [0/383] | loss: 0.5897 (avg: 0.0015) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 29 [100/383] | loss: 0.6117 (avg: 0.1705) | acc: 0.4453 (avg: 0.4766) |\n",
      "Epoch 29 [200/383] | loss: 0.6564 (avg: 0.3476) | acc: 0.5000 (avg: 0.4719) |\n",
      "Epoch 29 [300/383] | loss: 0.7576 (avg: 0.5235) | acc: 0.4453 (avg: 0.4691) |\n",
      "Validation | acc (nat): 0.6530 | acc (rob): 0.4610 |\n",
      "Epoch 30 [0/383] | loss: 0.6600 (avg: 0.0017) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 30 [100/383] | loss: 0.6088 (avg: 0.1650) | acc: 0.4922 (avg: 0.4755) |\n",
      "Epoch 30 [200/383] | loss: 0.5882 (avg: 0.3358) | acc: 0.4688 (avg: 0.4743) |\n",
      "Epoch 30 [300/383] | loss: 0.6440 (avg: 0.5131) | acc: 0.4453 (avg: 0.4719) |\n",
      "Validation | acc (nat): 0.6320 | acc (rob): 0.4280 |\n",
      "Epoch 31 [0/383] | loss: 0.7569 (avg: 0.0020) | acc: 0.3750 (avg: 0.3750) |\n",
      "Epoch 31 [100/383] | loss: 0.7179 (avg: 0.1741) | acc: 0.4219 (avg: 0.4715) |\n",
      "Epoch 31 [200/383] | loss: 0.5199 (avg: 0.3370) | acc: 0.5312 (avg: 0.4768) |\n",
      "Epoch 31 [300/383] | loss: 0.6347 (avg: 0.5044) | acc: 0.4219 (avg: 0.4768) |\n",
      "Validation | acc (nat): 0.6910 | acc (rob): 0.4650 |\n",
      "Epoch 32 [0/383] | loss: 0.5272 (avg: 0.0014) | acc: 0.5078 (avg: 0.5078) |\n",
      "Epoch 32 [100/383] | loss: 0.7011 (avg: 0.1669) | acc: 0.4609 (avg: 0.4746) |\n",
      "Epoch 32 [200/383] | loss: 0.6343 (avg: 0.3307) | acc: 0.4141 (avg: 0.4747) |\n",
      "Epoch 32 [300/383] | loss: 0.4331 (avg: 0.5057) | acc: 0.5391 (avg: 0.4720) |\n",
      "Validation | acc (nat): 0.6650 | acc (rob): 0.4670 |\n",
      "Epoch 33 [0/383] | loss: 0.5712 (avg: 0.0015) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 33 [100/383] | loss: 0.5409 (avg: 0.1618) | acc: 0.5312 (avg: 0.4797) |\n",
      "Epoch 33 [200/383] | loss: 0.5375 (avg: 0.3346) | acc: 0.4688 (avg: 0.4731) |\n",
      "Epoch 33 [300/383] | loss: 0.7497 (avg: 0.5039) | acc: 0.5000 (avg: 0.4726) |\n",
      "Validation | acc (nat): 0.6620 | acc (rob): 0.4390 |\n",
      "Epoch 34 [0/383] | loss: 0.6859 (avg: 0.0018) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 34 [100/383] | loss: 0.4763 (avg: 0.1687) | acc: 0.4922 (avg: 0.4774) |\n",
      "Epoch 34 [200/383] | loss: 0.5831 (avg: 0.3316) | acc: 0.4609 (avg: 0.4798) |\n",
      "Epoch 34 [300/383] | loss: 0.7719 (avg: 0.4999) | acc: 0.3828 (avg: 0.4788) |\n",
      "Validation | acc (nat): 0.6730 | acc (rob): 0.4590 |\n",
      "Epoch 35 [0/383] | loss: 0.5460 (avg: 0.0014) | acc: 0.5469 (avg: 0.5469) |\n",
      "Epoch 35 [100/383] | loss: 0.6843 (avg: 0.1660) | acc: 0.4688 (avg: 0.4780) |\n",
      "Epoch 35 [200/383] | loss: 0.6963 (avg: 0.3292) | acc: 0.4688 (avg: 0.4817) |\n",
      "Epoch 35 [300/383] | loss: 0.7589 (avg: 0.4971) | acc: 0.4219 (avg: 0.4793) |\n",
      "Validation | acc (nat): 0.6590 | acc (rob): 0.4800 |\n",
      "Epoch 36 [0/383] | loss: 0.5843 (avg: 0.0015) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 36 [100/383] | loss: 0.7054 (avg: 0.1577) | acc: 0.5547 (avg: 0.4847) |\n",
      "Epoch 36 [200/383] | loss: 0.7747 (avg: 0.3290) | acc: 0.4297 (avg: 0.4804) |\n",
      "Epoch 36 [300/383] | loss: 0.6500 (avg: 0.5000) | acc: 0.4219 (avg: 0.4779) |\n",
      "Validation | acc (nat): 0.6650 | acc (rob): 0.4980 |\n",
      "Epoch 37 [0/383] | loss: 0.6641 (avg: 0.0017) | acc: 0.3984 (avg: 0.3984) |\n",
      "Epoch 37 [100/383] | loss: 0.6135 (avg: 0.1556) | acc: 0.4375 (avg: 0.4907) |\n",
      "Epoch 37 [200/383] | loss: 0.5682 (avg: 0.3137) | acc: 0.4844 (avg: 0.4857) |\n",
      "Epoch 37 [300/383] | loss: 0.5049 (avg: 0.4692) | acc: 0.5391 (avg: 0.4862) |\n",
      "Validation | acc (nat): 0.6670 | acc (rob): 0.4530 |\n",
      "Epoch 38 [0/383] | loss: 0.5074 (avg: 0.0013) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 38 [100/383] | loss: 0.7528 (avg: 0.1570) | acc: 0.4609 (avg: 0.4884) |\n",
      "Epoch 38 [200/383] | loss: 0.4914 (avg: 0.3208) | acc: 0.5234 (avg: 0.4829) |\n",
      "Epoch 38 [300/383] | loss: 0.6475 (avg: 0.4875) | acc: 0.4688 (avg: 0.4815) |\n",
      "Validation | acc (nat): 0.6850 | acc (rob): 0.4800 |\n",
      "Epoch 39 [0/383] | loss: 0.7161 (avg: 0.0019) | acc: 0.3672 (avg: 0.3672) |\n",
      "Epoch 39 [100/383] | loss: 0.5358 (avg: 0.1530) | acc: 0.4766 (avg: 0.4869) |\n",
      "Epoch 39 [200/383] | loss: 0.6749 (avg: 0.3260) | acc: 0.4531 (avg: 0.4803) |\n",
      "Epoch 39 [300/383] | loss: 0.6743 (avg: 0.4891) | acc: 0.4688 (avg: 0.4819) |\n",
      "Validation | acc (nat): 0.7030 | acc (rob): 0.4950 |\n",
      "Epoch 40 [0/383] | loss: 0.6198 (avg: 0.0016) | acc: 0.5312 (avg: 0.5312) |\n",
      "Epoch 40 [100/383] | loss: 0.4141 (avg: 0.1621) | acc: 0.4766 (avg: 0.4843) |\n",
      "Epoch 40 [200/383] | loss: 0.5952 (avg: 0.3197) | acc: 0.4766 (avg: 0.4878) |\n",
      "Epoch 40 [300/383] | loss: 0.6906 (avg: 0.4765) | acc: 0.4688 (avg: 0.4877) |\n",
      "Validation | acc (nat): 0.7030 | acc (rob): 0.4670 |\n",
      "Epoch 41 [0/383] | loss: 0.5037 (avg: 0.0013) | acc: 0.5859 (avg: 0.5859) |\n",
      "Epoch 41 [100/383] | loss: 0.6482 (avg: 0.1696) | acc: 0.4609 (avg: 0.4785) |\n",
      "Epoch 41 [200/383] | loss: 0.6597 (avg: 0.3358) | acc: 0.5234 (avg: 0.4782) |\n",
      "Epoch 41 [300/383] | loss: 0.6713 (avg: 0.4880) | acc: 0.4766 (avg: 0.4842) |\n",
      "Validation | acc (nat): 0.6900 | acc (rob): 0.4830 |\n",
      "Epoch 42 [0/383] | loss: 0.6004 (avg: 0.0016) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 42 [100/383] | loss: 0.6778 (avg: 0.1583) | acc: 0.5078 (avg: 0.4957) |\n",
      "Epoch 42 [200/383] | loss: 0.7646 (avg: 0.3182) | acc: 0.4297 (avg: 0.4900) |\n",
      "Epoch 42 [300/383] | loss: 0.6987 (avg: 0.4875) | acc: 0.4922 (avg: 0.4862) |\n",
      "Validation | acc (nat): 0.6910 | acc (rob): 0.4760 |\n",
      "Epoch 43 [0/383] | loss: 0.3892 (avg: 0.0010) | acc: 0.5547 (avg: 0.5547) |\n",
      "Epoch 43 [100/383] | loss: 0.5776 (avg: 0.1509) | acc: 0.5078 (avg: 0.4996) |\n",
      "Epoch 43 [200/383] | loss: 0.6433 (avg: 0.3140) | acc: 0.4688 (avg: 0.4901) |\n",
      "Epoch 43 [300/383] | loss: 0.6261 (avg: 0.4658) | acc: 0.5078 (avg: 0.4902) |\n",
      "Validation | acc (nat): 0.6470 | acc (rob): 0.4640 |\n",
      "Epoch 44 [0/383] | loss: 0.4748 (avg: 0.0012) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 44 [100/383] | loss: 0.5785 (avg: 0.1543) | acc: 0.5312 (avg: 0.4937) |\n",
      "Epoch 44 [200/383] | loss: 0.5656 (avg: 0.3039) | acc: 0.5078 (avg: 0.4939) |\n",
      "Epoch 44 [300/383] | loss: 0.6319 (avg: 0.4649) | acc: 0.5000 (avg: 0.4909) |\n",
      "Validation | acc (nat): 0.6820 | acc (rob): 0.4810 |\n",
      "Epoch 45 [0/383] | loss: 0.4861 (avg: 0.0013) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 45 [100/383] | loss: 0.7162 (avg: 0.1485) | acc: 0.4922 (avg: 0.4978) |\n",
      "Epoch 45 [200/383] | loss: 0.6748 (avg: 0.3065) | acc: 0.4609 (avg: 0.4907) |\n",
      "Epoch 45 [300/383] | loss: 0.4627 (avg: 0.4680) | acc: 0.5078 (avg: 0.4887) |\n",
      "Validation | acc (nat): 0.6860 | acc (rob): 0.4640 |\n",
      "Epoch 46 [0/383] | loss: 0.4393 (avg: 0.0011) | acc: 0.5078 (avg: 0.5078) |\n",
      "Epoch 46 [100/383] | loss: 0.8078 (avg: 0.1499) | acc: 0.3906 (avg: 0.5013) |\n",
      "Epoch 46 [200/383] | loss: 0.5903 (avg: 0.3084) | acc: 0.4375 (avg: 0.4942) |\n",
      "Epoch 46 [300/383] | loss: 0.6676 (avg: 0.4674) | acc: 0.4766 (avg: 0.4926) |\n",
      "Validation | acc (nat): 0.6690 | acc (rob): 0.4570 |\n",
      "Epoch 47 [0/383] | loss: 0.5511 (avg: 0.0014) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 47 [100/383] | loss: 0.6074 (avg: 0.1532) | acc: 0.4609 (avg: 0.4985) |\n",
      "Epoch 47 [200/383] | loss: 0.5459 (avg: 0.3024) | acc: 0.4688 (avg: 0.4983) |\n",
      "Epoch 47 [300/383] | loss: 0.7575 (avg: 0.4599) | acc: 0.4766 (avg: 0.4951) |\n",
      "Validation | acc (nat): 0.6800 | acc (rob): 0.4950 |\n",
      "Epoch 48 [0/383] | loss: 0.4959 (avg: 0.0013) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 48 [100/383] | loss: 0.6652 (avg: 0.1558) | acc: 0.4844 (avg: 0.4959) |\n",
      "Epoch 48 [200/383] | loss: 0.6262 (avg: 0.3108) | acc: 0.4688 (avg: 0.4937) |\n",
      "Epoch 48 [300/383] | loss: 0.7036 (avg: 0.4685) | acc: 0.4453 (avg: 0.4917) |\n",
      "Validation | acc (nat): 0.6570 | acc (rob): 0.4850 |\n",
      "Epoch 49 [0/383] | loss: 0.5308 (avg: 0.0014) | acc: 0.5000 (avg: 0.5000) |\n",
      "Epoch 49 [100/383] | loss: 0.4993 (avg: 0.1502) | acc: 0.4609 (avg: 0.4976) |\n",
      "Epoch 49 [200/383] | loss: 0.6582 (avg: 0.3090) | acc: 0.4844 (avg: 0.4915) |\n",
      "Epoch 49 [300/383] | loss: 0.4401 (avg: 0.4667) | acc: 0.5781 (avg: 0.4899) |\n",
      "Validation | acc (nat): 0.7000 | acc (rob): 0.4850 |\n",
      "Epoch 50 [0/383] | loss: 0.4914 (avg: 0.0013) | acc: 0.4922 (avg: 0.4922) |\n",
      "Epoch 50 [100/383] | loss: 0.5511 (avg: 0.1463) | acc: 0.5000 (avg: 0.4960) |\n",
      "Epoch 50 [200/383] | loss: 0.6231 (avg: 0.3062) | acc: 0.4609 (avg: 0.4922) |\n",
      "Epoch 50 [300/383] | loss: 0.6209 (avg: 0.4610) | acc: 0.5156 (avg: 0.4912) |\n",
      "Validation | acc (nat): 0.7200 | acc (rob): 0.4970 |\n",
      "Epoch 51 [0/383] | loss: 0.6310 (avg: 0.0016) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 51 [100/383] | loss: 0.4550 (avg: 0.1449) | acc: 0.5078 (avg: 0.5096) |\n",
      "Epoch 51 [200/383] | loss: 0.4938 (avg: 0.3074) | acc: 0.5234 (avg: 0.4932) |\n",
      "Epoch 51 [300/383] | loss: 0.5541 (avg: 0.4609) | acc: 0.4844 (avg: 0.4913) |\n",
      "Validation | acc (nat): 0.7100 | acc (rob): 0.4920 |\n",
      "Epoch 52 [0/383] | loss: 0.4934 (avg: 0.0013) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 52 [100/383] | loss: 0.6364 (avg: 0.1471) | acc: 0.4375 (avg: 0.4997) |\n",
      "Epoch 52 [200/383] | loss: 0.4900 (avg: 0.2961) | acc: 0.5234 (avg: 0.4963) |\n",
      "Epoch 52 [300/383] | loss: 0.6815 (avg: 0.4538) | acc: 0.4141 (avg: 0.4935) |\n",
      "Validation | acc (nat): 0.6950 | acc (rob): 0.4900 |\n",
      "Epoch 53 [0/383] | loss: 0.6727 (avg: 0.0018) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 53 [100/383] | loss: 0.5626 (avg: 0.1516) | acc: 0.5547 (avg: 0.4997) |\n",
      "Epoch 53 [200/383] | loss: 0.5197 (avg: 0.2985) | acc: 0.5156 (avg: 0.4993) |\n",
      "Epoch 53 [300/383] | loss: 0.7345 (avg: 0.4491) | acc: 0.4609 (avg: 0.4968) |\n",
      "Validation | acc (nat): 0.6800 | acc (rob): 0.4730 |\n",
      "Epoch 54 [0/383] | loss: 0.5307 (avg: 0.0014) | acc: 0.5312 (avg: 0.5312) |\n",
      "Epoch 54 [100/383] | loss: 0.7270 (avg: 0.1488) | acc: 0.4688 (avg: 0.4990) |\n",
      "Epoch 54 [200/383] | loss: 0.4731 (avg: 0.2969) | acc: 0.4766 (avg: 0.5012) |\n",
      "Epoch 54 [300/383] | loss: 0.6870 (avg: 0.4515) | acc: 0.4141 (avg: 0.4977) |\n",
      "Validation | acc (nat): 0.6940 | acc (rob): 0.5010 |\n",
      "Epoch 55 [0/383] | loss: 0.6991 (avg: 0.0018) | acc: 0.5312 (avg: 0.5312) |\n",
      "Epoch 55 [100/383] | loss: 0.5607 (avg: 0.1456) | acc: 0.4688 (avg: 0.5046) |\n",
      "Epoch 55 [200/383] | loss: 0.5188 (avg: 0.2916) | acc: 0.5469 (avg: 0.5007) |\n",
      "Epoch 55 [300/383] | loss: 0.5608 (avg: 0.4454) | acc: 0.5312 (avg: 0.4977) |\n",
      "Validation | acc (nat): 0.6290 | acc (rob): 0.4430 |\n",
      "Epoch 56 [0/383] | loss: 0.7814 (avg: 0.0020) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 56 [100/383] | loss: 0.5077 (avg: 0.1456) | acc: 0.5938 (avg: 0.5051) |\n",
      "Epoch 56 [200/383] | loss: 0.5392 (avg: 0.2985) | acc: 0.4609 (avg: 0.4975) |\n",
      "Epoch 56 [300/383] | loss: 0.9067 (avg: 0.4519) | acc: 0.3906 (avg: 0.4949) |\n",
      "Validation | acc (nat): 0.6970 | acc (rob): 0.4790 |\n",
      "Epoch 57 [0/383] | loss: 0.6288 (avg: 0.0016) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 57 [100/383] | loss: 0.6276 (avg: 0.1496) | acc: 0.4844 (avg: 0.4957) |\n",
      "Epoch 57 [200/383] | loss: 0.5223 (avg: 0.2994) | acc: 0.4766 (avg: 0.4948) |\n",
      "Epoch 57 [300/383] | loss: 0.4481 (avg: 0.4655) | acc: 0.6016 (avg: 0.4889) |\n",
      "Validation | acc (nat): 0.6850 | acc (rob): 0.4660 |\n",
      "Epoch 58 [0/383] | loss: 0.6804 (avg: 0.0018) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 58 [100/383] | loss: 0.5428 (avg: 0.1504) | acc: 0.4688 (avg: 0.4983) |\n",
      "Epoch 58 [200/383] | loss: 0.4883 (avg: 0.3030) | acc: 0.5469 (avg: 0.4979) |\n",
      "Epoch 58 [300/383] | loss: 0.5822 (avg: 0.4508) | acc: 0.4453 (avg: 0.4972) |\n",
      "Validation | acc (nat): 0.7040 | acc (rob): 0.4780 |\n",
      "Epoch 59 [0/383] | loss: 0.5683 (avg: 0.0015) | acc: 0.5703 (avg: 0.5703) |\n",
      "Epoch 59 [100/383] | loss: 0.5608 (avg: 0.1439) | acc: 0.5234 (avg: 0.5017) |\n",
      "Epoch 59 [200/383] | loss: 0.5968 (avg: 0.2986) | acc: 0.5078 (avg: 0.4965) |\n",
      "Epoch 59 [300/383] | loss: 0.4450 (avg: 0.4493) | acc: 0.5156 (avg: 0.4965) |\n",
      "Validation | acc (nat): 0.6850 | acc (rob): 0.4650 |\n",
      "Epoch 60 [0/383] | loss: 0.5044 (avg: 0.0013) | acc: 0.5859 (avg: 0.5859) |\n",
      "Epoch 60 [100/383] | loss: 0.5119 (avg: 0.1429) | acc: 0.5078 (avg: 0.5066) |\n",
      "Epoch 60 [200/383] | loss: 0.7304 (avg: 0.2863) | acc: 0.3828 (avg: 0.5018) |\n",
      "Epoch 60 [300/383] | loss: 0.6431 (avg: 0.4458) | acc: 0.4688 (avg: 0.4971) |\n",
      "Validation | acc (nat): 0.7020 | acc (rob): 0.4820 |\n",
      "Epoch 61 [0/383] | loss: 0.5110 (avg: 0.0013) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 61 [100/383] | loss: 0.5919 (avg: 0.1350) | acc: 0.4531 (avg: 0.5112) |\n",
      "Epoch 61 [200/383] | loss: 0.4818 (avg: 0.2924) | acc: 0.5156 (avg: 0.4974) |\n",
      "Epoch 61 [300/383] | loss: 0.4885 (avg: 0.4379) | acc: 0.5234 (avg: 0.4997) |\n",
      "Validation | acc (nat): 0.6990 | acc (rob): 0.5060 |\n",
      "Epoch 62 [0/383] | loss: 0.6350 (avg: 0.0017) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 62 [100/383] | loss: 0.5128 (avg: 0.1416) | acc: 0.4922 (avg: 0.5073) |\n",
      "Epoch 62 [200/383] | loss: 0.3978 (avg: 0.2863) | acc: 0.5312 (avg: 0.5037) |\n",
      "Epoch 62 [300/383] | loss: 0.6881 (avg: 0.4361) | acc: 0.5156 (avg: 0.5020) |\n",
      "Validation | acc (nat): 0.7020 | acc (rob): 0.4970 |\n",
      "Epoch 63 [0/383] | loss: 0.4775 (avg: 0.0012) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 63 [100/383] | loss: 0.5850 (avg: 0.1442) | acc: 0.5625 (avg: 0.5111) |\n",
      "Epoch 63 [200/383] | loss: 0.6430 (avg: 0.3002) | acc: 0.4219 (avg: 0.4989) |\n",
      "Epoch 63 [300/383] | loss: 0.5912 (avg: 0.4495) | acc: 0.5000 (avg: 0.4987) |\n",
      "Validation | acc (nat): 0.7110 | acc (rob): 0.4770 |\n",
      "Epoch 64 [0/383] | loss: 0.5208 (avg: 0.0014) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 64 [100/383] | loss: 0.4496 (avg: 0.1467) | acc: 0.4688 (avg: 0.4952) |\n",
      "Epoch 64 [200/383] | loss: 0.4711 (avg: 0.3022) | acc: 0.4766 (avg: 0.4950) |\n",
      "Epoch 64 [300/383] | loss: 0.6829 (avg: 0.4522) | acc: 0.4141 (avg: 0.4953) |\n",
      "Validation | acc (nat): 0.6550 | acc (rob): 0.4770 |\n",
      "Epoch 65 [0/383] | loss: 0.4233 (avg: 0.0011) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 65 [100/383] | loss: 0.4488 (avg: 0.1439) | acc: 0.5156 (avg: 0.5032) |\n",
      "Epoch 65 [200/383] | loss: 0.5752 (avg: 0.2841) | acc: 0.5078 (avg: 0.5037) |\n",
      "Epoch 65 [300/383] | loss: 0.7640 (avg: 0.4396) | acc: 0.4141 (avg: 0.4995) |\n",
      "Validation | acc (nat): 0.7020 | acc (rob): 0.4560 |\n",
      "Epoch 66 [0/383] | loss: 0.6432 (avg: 0.0017) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 66 [100/383] | loss: 0.6653 (avg: 0.1426) | acc: 0.4766 (avg: 0.5047) |\n",
      "Epoch 66 [200/383] | loss: 0.6800 (avg: 0.2883) | acc: 0.4375 (avg: 0.5017) |\n",
      "Epoch 66 [300/383] | loss: 0.7143 (avg: 0.4416) | acc: 0.4219 (avg: 0.4998) |\n",
      "Validation | acc (nat): 0.6830 | acc (rob): 0.4690 |\n",
      "Epoch 67 [0/383] | loss: 0.5818 (avg: 0.0015) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 67 [100/383] | loss: 0.5814 (avg: 0.1380) | acc: 0.4453 (avg: 0.5135) |\n",
      "Epoch 67 [200/383] | loss: 0.5657 (avg: 0.2881) | acc: 0.5156 (avg: 0.5033) |\n",
      "Epoch 67 [300/383] | loss: 0.7338 (avg: 0.4422) | acc: 0.5000 (avg: 0.4973) |\n",
      "Validation | acc (nat): 0.7060 | acc (rob): 0.5000 |\n",
      "Epoch 68 [0/383] | loss: 0.5104 (avg: 0.0013) | acc: 0.5625 (avg: 0.5625) |\n",
      "Epoch 68 [100/383] | loss: 0.6861 (avg: 0.1478) | acc: 0.5000 (avg: 0.5016) |\n",
      "Epoch 68 [200/383] | loss: 0.7090 (avg: 0.2960) | acc: 0.4297 (avg: 0.4984) |\n",
      "Epoch 68 [300/383] | loss: 0.5588 (avg: 0.4473) | acc: 0.4531 (avg: 0.4970) |\n",
      "Validation | acc (nat): 0.6410 | acc (rob): 0.4600 |\n",
      "Epoch 69 [0/383] | loss: 0.7228 (avg: 0.0019) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 69 [100/383] | loss: 0.6340 (avg: 0.1434) | acc: 0.4531 (avg: 0.5050) |\n",
      "Epoch 69 [200/383] | loss: 0.6208 (avg: 0.2843) | acc: 0.4609 (avg: 0.5038) |\n",
      "Epoch 69 [300/383] | loss: 0.4856 (avg: 0.4312) | acc: 0.5781 (avg: 0.5027) |\n",
      "Validation | acc (nat): 0.6510 | acc (rob): 0.4760 |\n",
      "Epoch 70 [0/383] | loss: 0.7800 (avg: 0.0020) | acc: 0.4453 (avg: 0.4453) |\n",
      "Epoch 70 [100/383] | loss: 0.2908 (avg: 0.1455) | acc: 0.6172 (avg: 0.5063) |\n",
      "Epoch 70 [200/383] | loss: 0.5929 (avg: 0.2900) | acc: 0.5078 (avg: 0.5024) |\n",
      "Epoch 70 [300/383] | loss: 0.4922 (avg: 0.4428) | acc: 0.5000 (avg: 0.4998) |\n",
      "Validation | acc (nat): 0.6830 | acc (rob): 0.4860 |\n",
      "Epoch 71 [0/383] | loss: 0.4340 (avg: 0.0011) | acc: 0.5312 (avg: 0.5312) |\n",
      "Epoch 71 [100/383] | loss: 0.4670 (avg: 0.1342) | acc: 0.4766 (avg: 0.5130) |\n",
      "Epoch 71 [200/383] | loss: 0.8111 (avg: 0.2807) | acc: 0.4688 (avg: 0.5073) |\n",
      "Epoch 71 [300/383] | loss: 0.5304 (avg: 0.4319) | acc: 0.4766 (avg: 0.5019) |\n",
      "Validation | acc (nat): 0.7080 | acc (rob): 0.4810 |\n",
      "Epoch 72 [0/383] | loss: 0.6772 (avg: 0.0018) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 72 [100/383] | loss: 0.5217 (avg: 0.1415) | acc: 0.4844 (avg: 0.5060) |\n",
      "Epoch 72 [200/383] | loss: 0.4052 (avg: 0.2827) | acc: 0.4844 (avg: 0.5065) |\n",
      "Epoch 72 [300/383] | loss: 0.5281 (avg: 0.4274) | acc: 0.4844 (avg: 0.5050) |\n",
      "Validation | acc (nat): 0.6980 | acc (rob): 0.4650 |\n",
      "Epoch 73 [0/383] | loss: 0.5058 (avg: 0.0013) | acc: 0.4922 (avg: 0.4922) |\n",
      "Epoch 73 [100/383] | loss: 0.5875 (avg: 0.1508) | acc: 0.5156 (avg: 0.5030) |\n",
      "Epoch 73 [200/383] | loss: 0.5798 (avg: 0.2970) | acc: 0.4141 (avg: 0.5014) |\n",
      "Epoch 73 [300/383] | loss: 0.4787 (avg: 0.4389) | acc: 0.5312 (avg: 0.5036) |\n",
      "Validation | acc (nat): 0.6900 | acc (rob): 0.4770 |\n",
      "Epoch 74 [0/383] | loss: 0.6102 (avg: 0.0016) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 74 [100/383] | loss: 0.4242 (avg: 0.1472) | acc: 0.5234 (avg: 0.4952) |\n",
      "Epoch 74 [200/383] | loss: 0.6023 (avg: 0.2931) | acc: 0.5469 (avg: 0.5005) |\n",
      "Epoch 74 [300/383] | loss: 0.5489 (avg: 0.4434) | acc: 0.4766 (avg: 0.4995) |\n",
      "Validation | acc (nat): 0.6490 | acc (rob): 0.4720 |\n",
      "Epoch 75 [0/383] | loss: 0.6279 (avg: 0.0016) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 75 [100/383] | loss: 0.3441 (avg: 0.1153) | acc: 0.5781 (avg: 0.5381) |\n",
      "Epoch 75 [200/383] | loss: 0.1950 (avg: 0.2007) | acc: 0.6406 (avg: 0.5498) |\n",
      "Epoch 75 [300/383] | loss: 0.1448 (avg: 0.2710) | acc: 0.5859 (avg: 0.5602) |\n",
      "Validation | acc (nat): 0.7630 | acc (rob): 0.5440 |\n",
      "Epoch 76 [0/383] | loss: 0.2658 (avg: 0.0007) | acc: 0.5312 (avg: 0.5312) |\n",
      "Epoch 76 [100/383] | loss: 0.1962 (avg: 0.0565) | acc: 0.6016 (avg: 0.5769) |\n",
      "Epoch 76 [200/383] | loss: 0.2335 (avg: 0.1066) | acc: 0.5625 (avg: 0.5822) |\n",
      "Epoch 76 [300/383] | loss: 0.1736 (avg: 0.1542) | acc: 0.5547 (avg: 0.5850) |\n",
      "Validation | acc (nat): 0.7620 | acc (rob): 0.5680 |\n",
      "Epoch 77 [0/383] | loss: 0.2635 (avg: 0.0007) | acc: 0.5859 (avg: 0.5859) |\n",
      "Epoch 77 [100/383] | loss: 0.1737 (avg: 0.0427) | acc: 0.5625 (avg: 0.5897) |\n",
      "Epoch 77 [200/383] | loss: 0.1672 (avg: 0.0807) | acc: 0.5938 (avg: 0.5931) |\n",
      "Epoch 77 [300/383] | loss: 0.1399 (avg: 0.1125) | acc: 0.5703 (avg: 0.5947) |\n",
      "Validation | acc (nat): 0.7640 | acc (rob): 0.5590 |\n",
      "Epoch 78 [0/383] | loss: 0.2540 (avg: 0.0007) | acc: 0.5703 (avg: 0.5703) |\n",
      "Epoch 78 [100/383] | loss: -0.0126 (avg: 0.0281) | acc: 0.6328 (avg: 0.6023) |\n",
      "Epoch 78 [200/383] | loss: -0.0455 (avg: 0.0530) | acc: 0.6172 (avg: 0.6030) |\n",
      "Epoch 78 [300/383] | loss: 0.0129 (avg: 0.0818) | acc: 0.5938 (avg: 0.5996) |\n",
      "Validation | acc (nat): 0.7810 | acc (rob): 0.5710 |\n",
      "Epoch 79 [0/383] | loss: 0.0210 (avg: 0.0001) | acc: 0.6562 (avg: 0.6562) |\n",
      "Epoch 79 [100/383] | loss: -0.0760 (avg: 0.0177) | acc: 0.6406 (avg: 0.6127) |\n",
      "Epoch 79 [200/383] | loss: 0.0175 (avg: 0.0334) | acc: 0.5781 (avg: 0.6067) |\n",
      "Epoch 79 [300/383] | loss: 0.1612 (avg: 0.0457) | acc: 0.6406 (avg: 0.6056) |\n",
      "Validation | acc (nat): 0.7800 | acc (rob): 0.5640 |\n",
      "Epoch 80 [0/383] | loss: -0.0261 (avg: -0.0001) | acc: 0.6406 (avg: 0.6406) |\n",
      "Epoch 80 [100/383] | loss: 0.2426 (avg: 0.0179) | acc: 0.5469 (avg: 0.6046) |\n",
      "Epoch 80 [200/383] | loss: 0.1194 (avg: 0.0256) | acc: 0.5391 (avg: 0.6076) |\n",
      "Epoch 80 [300/383] | loss: 0.0796 (avg: 0.0357) | acc: 0.5703 (avg: 0.6086) |\n",
      "Validation | acc (nat): 0.7810 | acc (rob): 0.5510 |\n",
      "Epoch 81 [0/383] | loss: -0.0769 (avg: -0.0002) | acc: 0.6406 (avg: 0.6406) |\n",
      "Epoch 81 [100/383] | loss: -0.0218 (avg: -0.0023) | acc: 0.5938 (avg: 0.6177) |\n",
      "Epoch 81 [200/383] | loss: 0.0231 (avg: 0.0023) | acc: 0.5469 (avg: 0.6137) |\n",
      "Epoch 81 [300/383] | loss: 0.0252 (avg: 0.0048) | acc: 0.6172 (avg: 0.6126) |\n",
      "Validation | acc (nat): 0.7790 | acc (rob): 0.5390 |\n",
      "Epoch 82 [0/383] | loss: 0.0569 (avg: 0.0001) | acc: 0.6328 (avg: 0.6328) |\n",
      "Epoch 82 [100/383] | loss: 0.2724 (avg: -0.0004) | acc: 0.5391 (avg: 0.6125) |\n",
      "Epoch 82 [200/383] | loss: 0.1097 (avg: -0.0035) | acc: 0.6250 (avg: 0.6121) |\n",
      "Epoch 82 [300/383] | loss: 0.1286 (avg: -0.0103) | acc: 0.5625 (avg: 0.6114) |\n",
      "Validation | acc (nat): 0.7460 | acc (rob): 0.5310 |\n",
      "Epoch 83 [0/383] | loss: -0.0155 (avg: -0.0000) | acc: 0.6172 (avg: 0.6172) |\n",
      "Epoch 83 [100/383] | loss: -0.0777 (avg: 0.0028) | acc: 0.6172 (avg: 0.6047) |\n",
      "Epoch 83 [200/383] | loss: 0.0076 (avg: 0.0098) | acc: 0.5703 (avg: 0.6024) |\n",
      "Epoch 83 [300/383] | loss: 0.0480 (avg: 0.0141) | acc: 0.5859 (avg: 0.6039) |\n",
      "Validation | acc (nat): 0.7650 | acc (rob): 0.5540 |\n",
      "Epoch 84 [0/383] | loss: 0.0730 (avg: 0.0002) | acc: 0.5469 (avg: 0.5469) |\n",
      "Epoch 84 [100/383] | loss: -0.1616 (avg: -0.0196) | acc: 0.6484 (avg: 0.6189) |\n",
      "Epoch 84 [200/383] | loss: 0.0045 (avg: -0.0153) | acc: 0.5781 (avg: 0.6125) |\n",
      "Epoch 84 [300/383] | loss: -0.0594 (avg: -0.0127) | acc: 0.5703 (avg: 0.6100) |\n",
      "Validation | acc (nat): 0.7630 | acc (rob): 0.5500 |\n",
      "Epoch 85 [0/383] | loss: 0.1289 (avg: 0.0003) | acc: 0.5859 (avg: 0.5859) |\n",
      "Epoch 85 [100/383] | loss: -0.0194 (avg: -0.0069) | acc: 0.6016 (avg: 0.6151) |\n",
      "Epoch 85 [200/383] | loss: -0.2692 (avg: -0.0273) | acc: 0.7109 (avg: 0.6174) |\n",
      "Epoch 85 [300/383] | loss: 0.0085 (avg: -0.0173) | acc: 0.5781 (avg: 0.6113) |\n",
      "Validation | acc (nat): 0.7740 | acc (rob): 0.5350 |\n",
      "Epoch 86 [0/383] | loss: -0.1252 (avg: -0.0003) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 86 [100/383] | loss: 0.3091 (avg: -0.0177) | acc: 0.6406 (avg: 0.6195) |\n",
      "Epoch 86 [200/383] | loss: -0.0755 (avg: -0.0213) | acc: 0.6016 (avg: 0.6173) |\n",
      "Epoch 86 [300/383] | loss: -0.1517 (avg: -0.0207) | acc: 0.6094 (avg: 0.6153) |\n",
      "Validation | acc (nat): 0.7800 | acc (rob): 0.5550 |\n",
      "Epoch 87 [0/383] | loss: -0.0604 (avg: -0.0002) | acc: 0.5859 (avg: 0.5859) |\n",
      "Epoch 87 [100/383] | loss: 0.2183 (avg: 0.0074) | acc: 0.5234 (avg: 0.6050) |\n",
      "Epoch 87 [200/383] | loss: -0.1084 (avg: -0.0130) | acc: 0.5547 (avg: 0.6097) |\n",
      "Epoch 87 [300/383] | loss: -0.0233 (avg: -0.0188) | acc: 0.6172 (avg: 0.6098) |\n",
      "Validation | acc (nat): 0.7570 | acc (rob): 0.5440 |\n",
      "Epoch 88 [0/383] | loss: 0.0627 (avg: 0.0002) | acc: 0.5625 (avg: 0.5625) |\n",
      "Epoch 88 [100/383] | loss: -0.1939 (avg: -0.0098) | acc: 0.6797 (avg: 0.6156) |\n",
      "Epoch 88 [200/383] | loss: -0.2067 (avg: 0.0044) | acc: 0.6328 (avg: 0.6078) |\n",
      "Epoch 88 [300/383] | loss: -0.0660 (avg: -0.0016) | acc: 0.6250 (avg: 0.6085) |\n",
      "Validation | acc (nat): 0.7600 | acc (rob): 0.5500 |\n",
      "Epoch 89 [0/383] | loss: -0.0984 (avg: -0.0003) | acc: 0.6484 (avg: 0.6484) |\n",
      "Epoch 89 [100/383] | loss: 0.0751 (avg: -0.0141) | acc: 0.5703 (avg: 0.6129) |\n",
      "Epoch 89 [200/383] | loss: 0.1165 (avg: -0.0201) | acc: 0.6172 (avg: 0.6137) |\n",
      "Epoch 89 [300/383] | loss: 0.1886 (avg: -0.0347) | acc: 0.5625 (avg: 0.6132) |\n",
      "Validation | acc (nat): 0.7760 | acc (rob): 0.5450 |\n",
      "Epoch 90 [0/383] | loss: -0.1758 (avg: -0.0005) | acc: 0.6016 (avg: 0.6016) |\n",
      "Epoch 90 [100/383] | loss: -0.4930 (avg: -0.0485) | acc: 0.6484 (avg: 0.6402) |\n",
      "Epoch 90 [200/383] | loss: -0.2866 (avg: -0.1089) | acc: 0.6406 (avg: 0.6469) |\n",
      "Epoch 90 [300/383] | loss: -0.1617 (avg: -0.1742) | acc: 0.6172 (avg: 0.6455) |\n",
      "Validation | acc (nat): 0.8000 | acc (rob): 0.5720 |\n",
      "Epoch 91 [0/383] | loss: -0.2647 (avg: -0.0007) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 91 [100/383] | loss: -0.1082 (avg: -0.0716) | acc: 0.6641 (avg: 0.6548) |\n",
      "Epoch 91 [200/383] | loss: -0.3096 (avg: -0.1523) | acc: 0.6328 (avg: 0.6588) |\n",
      "Epoch 91 [300/383] | loss: -0.2494 (avg: -0.2310) | acc: 0.6328 (avg: 0.6591) |\n",
      "Validation | acc (nat): 0.8050 | acc (rob): 0.5700 |\n",
      "Epoch 92 [0/383] | loss: -0.1560 (avg: -0.0004) | acc: 0.6953 (avg: 0.6953) |\n",
      "Epoch 92 [100/383] | loss: -0.3556 (avg: -0.0911) | acc: 0.6172 (avg: 0.6648) |\n",
      "Epoch 92 [200/383] | loss: -0.3636 (avg: -0.1802) | acc: 0.6484 (avg: 0.6640) |\n",
      "Epoch 92 [300/383] | loss: -0.3300 (avg: -0.2664) | acc: 0.6562 (avg: 0.6650) |\n",
      "Validation | acc (nat): 0.7930 | acc (rob): 0.5820 |\n",
      "Epoch 93 [0/383] | loss: -0.4184 (avg: -0.0011) | acc: 0.6641 (avg: 0.6641) |\n",
      "Epoch 93 [100/383] | loss: -0.4571 (avg: -0.0885) | acc: 0.6719 (avg: 0.6655) |\n",
      "Epoch 93 [200/383] | loss: -0.4565 (avg: -0.1888) | acc: 0.6797 (avg: 0.6687) |\n",
      "Epoch 93 [300/383] | loss: 0.0870 (avg: -0.2867) | acc: 0.7031 (avg: 0.6686) |\n",
      "Validation | acc (nat): 0.8180 | acc (rob): 0.5840 |\n",
      "Epoch 94 [0/383] | loss: -0.2410 (avg: -0.0006) | acc: 0.7109 (avg: 0.7109) |\n",
      "Epoch 94 [100/383] | loss: -0.3361 (avg: -0.0980) | acc: 0.6484 (avg: 0.6705) |\n",
      "Epoch 94 [200/383] | loss: -0.2266 (avg: -0.1963) | acc: 0.6406 (avg: 0.6714) |\n",
      "Epoch 94 [300/383] | loss: -0.1462 (avg: -0.2941) | acc: 0.6797 (avg: 0.6691) |\n",
      "Validation | acc (nat): 0.8090 | acc (rob): 0.5500 |\n",
      "Epoch 95 [0/383] | loss: -0.5585 (avg: -0.0015) | acc: 0.6797 (avg: 0.6797) |\n",
      "Epoch 95 [100/383] | loss: -0.6660 (avg: -0.1039) | acc: 0.7578 (avg: 0.6675) |\n",
      "Epoch 95 [200/383] | loss: -0.2036 (avg: -0.2095) | acc: 0.6719 (avg: 0.6716) |\n",
      "Epoch 95 [300/383] | loss: -0.5213 (avg: -0.3144) | acc: 0.6406 (avg: 0.6722) |\n",
      "Validation | acc (nat): 0.8110 | acc (rob): 0.5750 |\n",
      "Epoch 96 [0/383] | loss: -0.4455 (avg: -0.0012) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 96 [100/383] | loss: -0.5153 (avg: -0.1038) | acc: 0.7344 (avg: 0.6709) |\n",
      "Epoch 96 [200/383] | loss: -0.5013 (avg: -0.2162) | acc: 0.7266 (avg: 0.6739) |\n",
      "Epoch 96 [300/383] | loss: -0.4999 (avg: -0.3288) | acc: 0.6797 (avg: 0.6753) |\n",
      "Validation | acc (nat): 0.8000 | acc (rob): 0.5820 |\n",
      "Epoch 97 [0/383] | loss: -0.6121 (avg: -0.0016) | acc: 0.7422 (avg: 0.7422) |\n",
      "Epoch 97 [100/383] | loss: -0.4480 (avg: -0.1191) | acc: 0.7031 (avg: 0.6875) |\n",
      "Epoch 97 [200/383] | loss: -0.2617 (avg: -0.2273) | acc: 0.5703 (avg: 0.6802) |\n",
      "Epoch 97 [300/383] | loss: -0.4347 (avg: -0.3419) | acc: 0.6953 (avg: 0.6778) |\n",
      "Validation | acc (nat): 0.8120 | acc (rob): 0.5480 |\n",
      "Epoch 98 [0/383] | loss: -0.4265 (avg: -0.0011) | acc: 0.6641 (avg: 0.6641) |\n",
      "Epoch 98 [100/383] | loss: -0.5461 (avg: -0.1138) | acc: 0.7109 (avg: 0.6767) |\n",
      "Epoch 98 [200/383] | loss: -0.4660 (avg: -0.2354) | acc: 0.6562 (avg: 0.6796) |\n",
      "Epoch 98 [300/383] | loss: -0.5342 (avg: -0.3513) | acc: 0.7031 (avg: 0.6799) |\n",
      "Validation | acc (nat): 0.8100 | acc (rob): 0.5670 |\n",
      "Epoch 99 [0/383] | loss: -0.5069 (avg: -0.0013) | acc: 0.6641 (avg: 0.6641) |\n",
      "Epoch 99 [100/383] | loss: -0.6740 (avg: -0.1233) | acc: 0.7188 (avg: 0.6854) |\n",
      "Epoch 99 [200/383] | loss: -0.3610 (avg: -0.2437) | acc: 0.6562 (avg: 0.6858) |\n",
      "Epoch 99 [300/383] | loss: 0.1690 (avg: -0.3686) | acc: 0.5938 (avg: 0.6847) |\n",
      "Validation | acc (nat): 0.7980 | acc (rob): 0.5860 |\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()])\n",
    "train_dataset, _ = get_dataloader(dataset, batch_size)\n",
    "num_samples = len(train_dataset)\n",
    "num_samples_for_train = int(num_samples * 0.98)\n",
    "num_samples_for_valid = num_samples - num_samples_for_train\n",
    "train_set, valid_set = random_split(train_dataset, [num_samples_for_train, num_samples_for_valid])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "model = nn.DataParallel(get_network(model_type, num_classes).cuda())\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = [75, 90]\n",
    "adjust_learning_rate = lr_scheduler.MultiStepLR(optimizer, scheduler, gamma=0.1)\n",
    "best_acc_nat, best_acc_rob = 0, 0\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    training(epoch, model, train_dataloader, optimizer, num_classes, beta, epsilon, alpha, num_repeats)\n",
    "    test_acc_nat, test_acc_rob = evaluation(epoch, model, valid_dataloader, alpha, epsilon, num_repeats)\n",
    "        \n",
    "    is_best = best_acc_nat < test_acc_nat and best_acc_rob < test_acc_rob\n",
    "    best_acc_nat = max(best_acc_nat, test_acc_nat)\n",
    "    best_acc_rob = max(best_acc_rob, test_acc_rob)\n",
    "    save_checkpoint = {'state_dict': model.state_dict(),\n",
    "                       'best_acc_nat': best_acc_nat,\n",
    "                       'best_acc_rob': best_acc_rob,\n",
    "                       'optimizer': optimizer.state_dict(),\n",
    "                       'model_type': model_type,\n",
    "                       'dataset': dataset}\n",
    "    torch.save(save_checkpoint, os.path.join(checkpoint, 'model'))\n",
    "    if is_best:\n",
    "        torch.save(save_checkpoint, os.path.join(checkpoint, 'best_model'))\n",
    "    adjust_learning_rate.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67273057-ddb6-4294-b7ae-3aa932ab0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35369705-fa42-4434-8f91-6cf99823f441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
