{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be933234-4b51-41a5-a5a0-9b831b916f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Margin-Awere Instance Learning (MAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85eb3db2-6a80-4e98-8a62-4fd714fc9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import yaml\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66240c81-1be4-4a34-af22-21a6292e0ff9",
   "metadata": {},
   "source": [
    "## Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01de849e-a804-4062-aef7-4cfcd5999b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '4,5,6,7'\n",
    "dataset = 'cifar10'\n",
    "model_type = 'wrn34-10'\n",
    "checkpoint = './checkpoint/mail/%s/%s' % (dataset, model_type)\n",
    "num_classes = 10\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 7e-4\n",
    "batch_size = 128\n",
    "total_epochs = 100\n",
    "pm_type = 'adv'\n",
    "gamma = 30\n",
    "beta = -0.07\n",
    "epsilon = 8/255\n",
    "alpha = 2/255\n",
    "num_repeats = 10\n",
    "warm_up = 74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c102405-4d46-45e0-b957-9e7022b7ae2f",
   "metadata": {},
   "source": [
    "## Inner maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee44a0f-0acc-41c3-95a0-e477e167e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats):\n",
    "    noise = torch.FloatTensor(inputs.shape).uniform_(-epsilon, epsilon).cuda()\n",
    "    x = torch.clamp(inputs + noise, min=0, max=1)\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        x.requires_grad_()\n",
    "        logits = model(x)\n",
    "        loss = xent(logits, targets)\n",
    "        loss.backward()\n",
    "        grads = x.grad.data\n",
    "        x = x.detach() + alpha*torch.sign(grads).detach()\n",
    "        x = torch.min(torch.max(x, inputs-epsilon), inputs+epsilon).clamp(min=0, max=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2591b-4ebf-4714-a3f4-85cdac7f272d",
   "metadata": {},
   "source": [
    "## Training (Outer minimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2276c9-d52b-4544-8448-42817161084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, optimizer, num_classes, gamma, \n",
    "             beta, pm_type, warm_up, epsilon=8/255, alpha=2/255, num_repeats=10):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "        \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        batch = inputs.size(0)\n",
    "        \n",
    "        x = inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "        logits = model(x)\n",
    "        if pm_type == 'nat':\n",
    "            probs = model(inputs).softmax(dim=1)\n",
    "        elif pm_type == 'adv':\n",
    "            probs = logits.softmax(dim=1)\n",
    "        \n",
    "        class_index = torch.arange(num_classes)[None,:].repeat(batch,1).cuda()\n",
    "        false_probs = probs[class_index!=targets[:,None]].view(batch, num_classes-1)\n",
    "        gt_probs = probs[class_index==targets[:,None]].unsqueeze(1)\n",
    "        top2_probs = torch.topk(false_probs, k=1).values\n",
    "        pm = gt_probs - top2_probs\n",
    "        s = sigmoid(-gamma*(pm - beta))\n",
    "        s = s/torch.sum(s)\n",
    "        \n",
    "        if warm_up < epoch:\n",
    "            loss = -torch.sum(s*F.log_softmax(logits, dim=1)[class_index==targets[:,None]])/batch\n",
    "        else:\n",
    "            loss = xent(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total += batch\n",
    "        total_loss += loss.item()\n",
    "        num_correct = torch.argmax(logits.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct += num_correct\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Epoch %d [%d/%d] | loss: %.4f (avg: %.4f) | acc: %.4f (avg: %.4f) |'\\\n",
    "                  % (epoch, idx, len(dataloader), loss.item(), total_loss/len(dataloader),\n",
    "                     num_correct/batch, total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1c8d2-8e9c-441c-b471-9eb237eb3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch, model, dataloader, alpha, epsilon, num_repeats):\n",
    "    model.eval()\n",
    "    total_correct_nat = 0\n",
    "    total_correct_adv = 0\n",
    "    \n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for samples in dataloader:\n",
    "        inputs, targets = samples[0].cuda(), samples[1].cuda()\n",
    "        batch = inputs.size(0)\n",
    "        with torch.enable_grad():\n",
    "            x = inner_max(model, xent, inputs, targets, epsilon, alpha, num_repeats)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits_nat = model(inputs)\n",
    "            logits_adv = model(x)\n",
    "        \n",
    "        total_correct_nat += torch.argmax(logits_nat.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct_adv += torch.argmax(logits_adv.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        \n",
    "    print('Validation | acc (nat): %.4f | acc (rob): %.4f |' % (total_correct_nat / len(dataloader.dataset),\n",
    "                                                                total_correct_adv / len(dataloader.dataset)))\n",
    "    return (total_correct_nat / len(dataloader.dataset)), (total_correct_adv / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208f000e-cb35-4ad5-93d5-fe47f788b266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0 [0/383] | loss: 3.1076 (avg: 0.0081) | acc: 0.0000 (avg: 0.0000) |\n",
      "Epoch 0 [100/383] | loss: 2.2392 (avg: 0.6147) | acc: 0.2031 (avg: 0.1400) |\n",
      "Epoch 0 [200/383] | loss: 2.1105 (avg: 1.1721) | acc: 0.1953 (avg: 0.1709) |\n",
      "Epoch 0 [300/383] | loss: 2.1858 (avg: 1.7149) | acc: 0.2422 (avg: 0.1900) |\n",
      "Validation | acc (nat): 0.3480 | acc (rob): 0.2660 |\n",
      "Epoch 1 [0/383] | loss: 2.0963 (avg: 0.0055) | acc: 0.2266 (avg: 0.2266) |\n",
      "Epoch 1 [100/383] | loss: 1.9475 (avg: 0.5329) | acc: 0.2891 (avg: 0.2487) |\n",
      "Epoch 1 [200/383] | loss: 2.0430 (avg: 1.0562) | acc: 0.2031 (avg: 0.2503) |\n",
      "Epoch 1 [300/383] | loss: 1.9933 (avg: 1.5680) | acc: 0.2500 (avg: 0.2550) |\n",
      "Validation | acc (nat): 0.4290 | acc (rob): 0.2980 |\n",
      "Epoch 2 [0/383] | loss: 1.9728 (avg: 0.0052) | acc: 0.2188 (avg: 0.2188) |\n",
      "Epoch 2 [100/383] | loss: 1.9202 (avg: 0.5141) | acc: 0.2109 (avg: 0.2714) |\n",
      "Epoch 2 [200/383] | loss: 1.9506 (avg: 1.0142) | acc: 0.2891 (avg: 0.2772) |\n",
      "Epoch 2 [300/383] | loss: 1.9257 (avg: 1.5031) | acc: 0.2812 (avg: 0.2857) |\n",
      "Validation | acc (nat): 0.4670 | acc (rob): 0.2970 |\n",
      "Epoch 3 [0/383] | loss: 1.7805 (avg: 0.0046) | acc: 0.3516 (avg: 0.3516) |\n",
      "Epoch 3 [100/383] | loss: 1.9190 (avg: 0.4848) | acc: 0.2422 (avg: 0.3083) |\n",
      "Epoch 3 [200/383] | loss: 1.8056 (avg: 0.9658) | acc: 0.3125 (avg: 0.3103) |\n",
      "Epoch 3 [300/383] | loss: 1.7636 (avg: 1.4399) | acc: 0.3203 (avg: 0.3146) |\n",
      "Validation | acc (nat): 0.5290 | acc (rob): 0.3440 |\n",
      "Epoch 4 [0/383] | loss: 1.7429 (avg: 0.0046) | acc: 0.3516 (avg: 0.3516) |\n",
      "Epoch 4 [100/383] | loss: 1.9158 (avg: 0.4673) | acc: 0.2656 (avg: 0.3410) |\n",
      "Epoch 4 [200/383] | loss: 1.7033 (avg: 0.9299) | acc: 0.3594 (avg: 0.3373) |\n",
      "Epoch 4 [300/383] | loss: 1.8059 (avg: 1.3860) | acc: 0.3047 (avg: 0.3400) |\n",
      "Validation | acc (nat): 0.6000 | acc (rob): 0.3940 |\n",
      "Epoch 5 [0/383] | loss: 1.7375 (avg: 0.0045) | acc: 0.4062 (avg: 0.4062) |\n",
      "Epoch 5 [100/383] | loss: 1.7030 (avg: 0.4561) | acc: 0.3594 (avg: 0.3550) |\n",
      "Epoch 5 [200/383] | loss: 1.5911 (avg: 0.9055) | acc: 0.4453 (avg: 0.3556) |\n",
      "Epoch 5 [300/383] | loss: 1.7362 (avg: 1.3491) | acc: 0.3047 (avg: 0.3580) |\n",
      "Validation | acc (nat): 0.6560 | acc (rob): 0.4040 |\n",
      "Epoch 6 [0/383] | loss: 1.5427 (avg: 0.0040) | acc: 0.4219 (avg: 0.4219) |\n",
      "Epoch 6 [100/383] | loss: 1.5826 (avg: 0.4456) | acc: 0.4453 (avg: 0.3621) |\n",
      "Epoch 6 [200/383] | loss: 1.5918 (avg: 0.8842) | acc: 0.4141 (avg: 0.3706) |\n",
      "Epoch 6 [300/383] | loss: 1.5601 (avg: 1.3200) | acc: 0.4297 (avg: 0.3732) |\n",
      "Validation | acc (nat): 0.6240 | acc (rob): 0.3980 |\n",
      "Epoch 7 [0/383] | loss: 1.7144 (avg: 0.0045) | acc: 0.3516 (avg: 0.3516) |\n",
      "Epoch 7 [100/383] | loss: 1.7007 (avg: 0.4359) | acc: 0.3906 (avg: 0.3862) |\n",
      "Epoch 7 [200/383] | loss: 1.6431 (avg: 0.8609) | acc: 0.3672 (avg: 0.3905) |\n",
      "Epoch 7 [300/383] | loss: 1.6199 (avg: 1.2913) | acc: 0.3984 (avg: 0.3875) |\n",
      "Validation | acc (nat): 0.6820 | acc (rob): 0.4190 |\n",
      "Epoch 8 [0/383] | loss: 1.5326 (avg: 0.0040) | acc: 0.4453 (avg: 0.4453) |\n",
      "Epoch 8 [100/383] | loss: 1.6993 (avg: 0.4279) | acc: 0.3672 (avg: 0.3861) |\n",
      "Epoch 8 [200/383] | loss: 1.6369 (avg: 0.8486) | acc: 0.4297 (avg: 0.3898) |\n",
      "Epoch 8 [300/383] | loss: 1.6843 (avg: 1.2706) | acc: 0.3750 (avg: 0.3913) |\n",
      "Validation | acc (nat): 0.6780 | acc (rob): 0.4240 |\n",
      "Epoch 9 [0/383] | loss: 1.5681 (avg: 0.0041) | acc: 0.4062 (avg: 0.4062) |\n",
      "Epoch 9 [100/383] | loss: 1.4962 (avg: 0.4246) | acc: 0.4219 (avg: 0.3948) |\n",
      "Epoch 9 [200/383] | loss: 1.5674 (avg: 0.8387) | acc: 0.4219 (avg: 0.4031) |\n",
      "Epoch 9 [300/383] | loss: 1.5794 (avg: 1.2578) | acc: 0.4062 (avg: 0.4016) |\n",
      "Validation | acc (nat): 0.6880 | acc (rob): 0.4310 |\n",
      "Epoch 10 [0/383] | loss: 1.5266 (avg: 0.0040) | acc: 0.4375 (avg: 0.4375) |\n",
      "Epoch 10 [100/383] | loss: 1.6057 (avg: 0.4176) | acc: 0.3750 (avg: 0.4090) |\n",
      "Epoch 10 [200/383] | loss: 1.5554 (avg: 0.8269) | acc: 0.4375 (avg: 0.4117) |\n",
      "Epoch 10 [300/383] | loss: 1.6063 (avg: 1.2390) | acc: 0.3750 (avg: 0.4108) |\n",
      "Validation | acc (nat): 0.6730 | acc (rob): 0.4510 |\n",
      "Epoch 11 [0/383] | loss: 1.4625 (avg: 0.0038) | acc: 0.4375 (avg: 0.4375) |\n",
      "Epoch 11 [100/383] | loss: 1.6147 (avg: 0.4113) | acc: 0.3750 (avg: 0.4155) |\n",
      "Epoch 11 [200/383] | loss: 1.5672 (avg: 0.8182) | acc: 0.3828 (avg: 0.4167) |\n",
      "Epoch 11 [300/383] | loss: 1.5580 (avg: 1.2280) | acc: 0.4375 (avg: 0.4161) |\n",
      "Validation | acc (nat): 0.6720 | acc (rob): 0.4140 |\n",
      "Epoch 12 [0/383] | loss: 1.4974 (avg: 0.0039) | acc: 0.4219 (avg: 0.4219) |\n",
      "Epoch 12 [100/383] | loss: 1.6310 (avg: 0.4049) | acc: 0.4609 (avg: 0.4236) |\n",
      "Epoch 12 [200/383] | loss: 1.5413 (avg: 0.8132) | acc: 0.3906 (avg: 0.4196) |\n",
      "Epoch 12 [300/383] | loss: 1.4807 (avg: 1.2157) | acc: 0.4375 (avg: 0.4198) |\n",
      "Validation | acc (nat): 0.7230 | acc (rob): 0.4380 |\n",
      "Epoch 13 [0/383] | loss: 1.5100 (avg: 0.0039) | acc: 0.3750 (avg: 0.3750) |\n",
      "Epoch 13 [100/383] | loss: 1.4599 (avg: 0.4035) | acc: 0.4297 (avg: 0.4288) |\n",
      "Epoch 13 [200/383] | loss: 1.4424 (avg: 0.8052) | acc: 0.4531 (avg: 0.4270) |\n",
      "Epoch 13 [300/383] | loss: 1.5493 (avg: 1.2095) | acc: 0.4219 (avg: 0.4258) |\n",
      "Validation | acc (nat): 0.7160 | acc (rob): 0.4200 |\n",
      "Epoch 14 [0/383] | loss: 1.5442 (avg: 0.0040) | acc: 0.4219 (avg: 0.4219) |\n",
      "Epoch 14 [100/383] | loss: 1.3923 (avg: 0.3978) | acc: 0.5078 (avg: 0.4374) |\n",
      "Epoch 14 [200/383] | loss: 1.6088 (avg: 0.7944) | acc: 0.4453 (avg: 0.4357) |\n",
      "Epoch 14 [300/383] | loss: 1.6380 (avg: 1.1940) | acc: 0.4219 (avg: 0.4350) |\n",
      "Validation | acc (nat): 0.6910 | acc (rob): 0.4420 |\n",
      "Epoch 15 [0/383] | loss: 1.5034 (avg: 0.0039) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 15 [100/383] | loss: 1.5472 (avg: 0.3976) | acc: 0.4453 (avg: 0.4384) |\n",
      "Epoch 15 [200/383] | loss: 1.5456 (avg: 0.7912) | acc: 0.4141 (avg: 0.4378) |\n",
      "Epoch 15 [300/383] | loss: 1.5146 (avg: 1.1861) | acc: 0.4297 (avg: 0.4383) |\n",
      "Validation | acc (nat): 0.7060 | acc (rob): 0.4660 |\n",
      "Epoch 16 [0/383] | loss: 1.5167 (avg: 0.0040) | acc: 0.3828 (avg: 0.3828) |\n",
      "Epoch 16 [100/383] | loss: 1.5077 (avg: 0.3941) | acc: 0.4297 (avg: 0.4474) |\n",
      "Epoch 16 [200/383] | loss: 1.5184 (avg: 0.7867) | acc: 0.4297 (avg: 0.4415) |\n",
      "Epoch 16 [300/383] | loss: 1.5297 (avg: 1.1789) | acc: 0.4297 (avg: 0.4417) |\n",
      "Validation | acc (nat): 0.7480 | acc (rob): 0.4700 |\n",
      "Epoch 17 [0/383] | loss: 1.4417 (avg: 0.0038) | acc: 0.4453 (avg: 0.4453) |\n",
      "Epoch 17 [100/383] | loss: 1.4609 (avg: 0.3904) | acc: 0.4375 (avg: 0.4439) |\n",
      "Epoch 17 [200/383] | loss: 1.4128 (avg: 0.7811) | acc: 0.4922 (avg: 0.4442) |\n",
      "Epoch 17 [300/383] | loss: 1.3924 (avg: 1.1716) | acc: 0.4609 (avg: 0.4439) |\n",
      "Validation | acc (nat): 0.7210 | acc (rob): 0.4700 |\n",
      "Epoch 18 [0/383] | loss: 1.6037 (avg: 0.0042) | acc: 0.3828 (avg: 0.3828) |\n",
      "Epoch 18 [100/383] | loss: 1.5453 (avg: 0.3928) | acc: 0.4609 (avg: 0.4459) |\n",
      "Epoch 18 [200/383] | loss: 1.3731 (avg: 0.7778) | acc: 0.4766 (avg: 0.4465) |\n",
      "Epoch 18 [300/383] | loss: 1.4790 (avg: 1.1638) | acc: 0.4219 (avg: 0.4469) |\n",
      "Validation | acc (nat): 0.7150 | acc (rob): 0.4790 |\n",
      "Epoch 19 [0/383] | loss: 1.4097 (avg: 0.0037) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 19 [100/383] | loss: 1.4392 (avg: 0.3860) | acc: 0.4141 (avg: 0.4595) |\n",
      "Epoch 19 [200/383] | loss: 1.3448 (avg: 0.7726) | acc: 0.5391 (avg: 0.4535) |\n",
      "Epoch 19 [300/383] | loss: 1.4501 (avg: 1.1602) | acc: 0.4688 (avg: 0.4502) |\n",
      "Validation | acc (nat): 0.7290 | acc (rob): 0.4700 |\n",
      "Epoch 20 [0/383] | loss: 1.5364 (avg: 0.0040) | acc: 0.4297 (avg: 0.4297) |\n",
      "Epoch 20 [100/383] | loss: 1.3489 (avg: 0.3840) | acc: 0.5469 (avg: 0.4539) |\n",
      "Epoch 20 [200/383] | loss: 1.4120 (avg: 0.7663) | acc: 0.5000 (avg: 0.4539) |\n",
      "Epoch 20 [300/383] | loss: 1.4414 (avg: 1.1514) | acc: 0.4766 (avg: 0.4534) |\n",
      "Validation | acc (nat): 0.7430 | acc (rob): 0.4680 |\n",
      "Epoch 21 [0/383] | loss: 1.4802 (avg: 0.0039) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 21 [100/383] | loss: 1.5753 (avg: 0.3840) | acc: 0.4219 (avg: 0.4524) |\n",
      "Epoch 21 [200/383] | loss: 1.4169 (avg: 0.7664) | acc: 0.4609 (avg: 0.4516) |\n",
      "Epoch 21 [300/383] | loss: 1.5117 (avg: 1.1485) | acc: 0.4062 (avg: 0.4498) |\n",
      "Validation | acc (nat): 0.7130 | acc (rob): 0.4830 |\n",
      "Epoch 22 [0/383] | loss: 1.4106 (avg: 0.0037) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 22 [100/383] | loss: 1.5594 (avg: 0.3814) | acc: 0.4141 (avg: 0.4637) |\n",
      "Epoch 22 [200/383] | loss: 1.3852 (avg: 0.7630) | acc: 0.5078 (avg: 0.4563) |\n",
      "Epoch 22 [300/383] | loss: 1.4907 (avg: 1.1431) | acc: 0.5000 (avg: 0.4557) |\n",
      "Validation | acc (nat): 0.7560 | acc (rob): 0.4730 |\n",
      "Epoch 23 [0/383] | loss: 1.3809 (avg: 0.0036) | acc: 0.5156 (avg: 0.5156) |\n",
      "Epoch 23 [100/383] | loss: 1.4079 (avg: 0.3821) | acc: 0.4766 (avg: 0.4567) |\n",
      "Epoch 23 [200/383] | loss: 1.4222 (avg: 0.7604) | acc: 0.4531 (avg: 0.4567) |\n",
      "Epoch 23 [300/383] | loss: 1.4808 (avg: 1.1452) | acc: 0.4922 (avg: 0.4536) |\n",
      "Validation | acc (nat): 0.7540 | acc (rob): 0.4850 |\n",
      "Epoch 24 [0/383] | loss: 1.3901 (avg: 0.0036) | acc: 0.5078 (avg: 0.5078) |\n",
      "Epoch 24 [100/383] | loss: 1.4137 (avg: 0.3791) | acc: 0.4844 (avg: 0.4667) |\n",
      "Epoch 24 [200/383] | loss: 1.4984 (avg: 0.7531) | acc: 0.4453 (avg: 0.4668) |\n",
      "Epoch 24 [300/383] | loss: 1.5059 (avg: 1.1345) | acc: 0.3984 (avg: 0.4625) |\n",
      "Validation | acc (nat): 0.7560 | acc (rob): 0.4940 |\n",
      "Epoch 25 [0/383] | loss: 1.4150 (avg: 0.0037) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 25 [100/383] | loss: 1.4598 (avg: 0.3767) | acc: 0.4531 (avg: 0.4669) |\n",
      "Epoch 25 [200/383] | loss: 1.5069 (avg: 0.7491) | acc: 0.4297 (avg: 0.4660) |\n",
      "Epoch 25 [300/383] | loss: 1.4536 (avg: 1.1303) | acc: 0.4609 (avg: 0.4638) |\n",
      "Validation | acc (nat): 0.7230 | acc (rob): 0.4550 |\n",
      "Epoch 26 [0/383] | loss: 1.4917 (avg: 0.0039) | acc: 0.4141 (avg: 0.4141) |\n",
      "Epoch 26 [100/383] | loss: 1.5165 (avg: 0.3751) | acc: 0.4453 (avg: 0.4707) |\n",
      "Epoch 26 [200/383] | loss: 1.5813 (avg: 0.7511) | acc: 0.3906 (avg: 0.4679) |\n",
      "Epoch 26 [300/383] | loss: 1.4743 (avg: 1.1287) | acc: 0.4609 (avg: 0.4652) |\n",
      "Validation | acc (nat): 0.7400 | acc (rob): 0.4900 |\n",
      "Epoch 27 [0/383] | loss: 1.4331 (avg: 0.0037) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 27 [100/383] | loss: 1.4057 (avg: 0.3744) | acc: 0.4844 (avg: 0.4704) |\n",
      "Epoch 27 [200/383] | loss: 1.5344 (avg: 0.7487) | acc: 0.4688 (avg: 0.4673) |\n",
      "Epoch 27 [300/383] | loss: 1.5661 (avg: 1.1261) | acc: 0.3984 (avg: 0.4637) |\n",
      "Validation | acc (nat): 0.7450 | acc (rob): 0.4740 |\n",
      "Epoch 28 [0/383] | loss: 1.3939 (avg: 0.0036) | acc: 0.4844 (avg: 0.4844) |\n",
      "Epoch 28 [100/383] | loss: 1.5124 (avg: 0.3754) | acc: 0.4141 (avg: 0.4665) |\n",
      "Epoch 28 [200/383] | loss: 1.5011 (avg: 0.7507) | acc: 0.4531 (avg: 0.4620) |\n",
      "Epoch 28 [300/383] | loss: 1.5614 (avg: 1.1259) | acc: 0.4766 (avg: 0.4625) |\n",
      "Validation | acc (nat): 0.7410 | acc (rob): 0.4680 |\n",
      "Epoch 29 [0/383] | loss: 1.3763 (avg: 0.0036) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 29 [100/383] | loss: 1.3661 (avg: 0.3746) | acc: 0.4844 (avg: 0.4640) |\n",
      "Epoch 29 [200/383] | loss: 1.3241 (avg: 0.7432) | acc: 0.5000 (avg: 0.4664) |\n",
      "Epoch 29 [300/383] | loss: 1.3232 (avg: 1.1187) | acc: 0.5156 (avg: 0.4653) |\n",
      "Validation | acc (nat): 0.7360 | acc (rob): 0.4740 |\n",
      "Epoch 30 [0/383] | loss: 1.5846 (avg: 0.0041) | acc: 0.4141 (avg: 0.4141) |\n",
      "Epoch 30 [100/383] | loss: 1.4653 (avg: 0.3750) | acc: 0.4531 (avg: 0.4705) |\n",
      "Epoch 30 [200/383] | loss: 1.3929 (avg: 0.7453) | acc: 0.4766 (avg: 0.4655) |\n",
      "Epoch 30 [300/383] | loss: 1.4526 (avg: 1.1168) | acc: 0.4922 (avg: 0.4660) |\n",
      "Validation | acc (nat): 0.7420 | acc (rob): 0.4550 |\n",
      "Epoch 31 [0/383] | loss: 1.3515 (avg: 0.0035) | acc: 0.5078 (avg: 0.5078) |\n",
      "Epoch 31 [100/383] | loss: 1.5075 (avg: 0.3699) | acc: 0.3906 (avg: 0.4775) |\n",
      "Epoch 31 [200/383] | loss: 1.3455 (avg: 0.7426) | acc: 0.5000 (avg: 0.4726) |\n",
      "Epoch 31 [300/383] | loss: 1.4150 (avg: 1.1141) | acc: 0.4297 (avg: 0.4714) |\n",
      "Validation | acc (nat): 0.7700 | acc (rob): 0.5090 |\n",
      "Epoch 32 [0/383] | loss: 1.4907 (avg: 0.0039) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 32 [100/383] | loss: 1.3296 (avg: 0.3682) | acc: 0.4844 (avg: 0.4772) |\n",
      "Epoch 32 [200/383] | loss: 1.4733 (avg: 0.7388) | acc: 0.3828 (avg: 0.4708) |\n",
      "Epoch 32 [300/383] | loss: 1.4256 (avg: 1.1088) | acc: 0.4688 (avg: 0.4706) |\n",
      "Validation | acc (nat): 0.7540 | acc (rob): 0.4690 |\n",
      "Epoch 33 [0/383] | loss: 1.4323 (avg: 0.0037) | acc: 0.4375 (avg: 0.4375) |\n",
      "Epoch 33 [100/383] | loss: 1.3857 (avg: 0.3695) | acc: 0.5078 (avg: 0.4713) |\n",
      "Epoch 33 [200/383] | loss: 1.4559 (avg: 0.7382) | acc: 0.4062 (avg: 0.4700) |\n",
      "Epoch 33 [300/383] | loss: 1.3135 (avg: 1.1122) | acc: 0.5078 (avg: 0.4693) |\n",
      "Validation | acc (nat): 0.7520 | acc (rob): 0.4750 |\n",
      "Epoch 34 [0/383] | loss: 1.4553 (avg: 0.0038) | acc: 0.4766 (avg: 0.4766) |\n",
      "Epoch 34 [100/383] | loss: 1.3252 (avg: 0.3712) | acc: 0.5312 (avg: 0.4735) |\n",
      "Epoch 34 [200/383] | loss: 1.6115 (avg: 0.7395) | acc: 0.3516 (avg: 0.4742) |\n",
      "Epoch 34 [300/383] | loss: 1.5486 (avg: 1.1106) | acc: 0.4141 (avg: 0.4733) |\n",
      "Validation | acc (nat): 0.7580 | acc (rob): 0.4850 |\n",
      "Epoch 35 [0/383] | loss: 1.3651 (avg: 0.0036) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 35 [100/383] | loss: 1.5846 (avg: 0.3687) | acc: 0.4922 (avg: 0.4775) |\n",
      "Epoch 35 [200/383] | loss: 1.4351 (avg: 0.7382) | acc: 0.4844 (avg: 0.4729) |\n",
      "Epoch 35 [300/383] | loss: 1.3406 (avg: 1.1094) | acc: 0.4531 (avg: 0.4716) |\n",
      "Validation | acc (nat): 0.7380 | acc (rob): 0.4770 |\n",
      "Epoch 36 [0/383] | loss: 1.5586 (avg: 0.0041) | acc: 0.4297 (avg: 0.4297) |\n",
      "Epoch 36 [100/383] | loss: 1.4222 (avg: 0.3684) | acc: 0.4844 (avg: 0.4773) |\n",
      "Epoch 36 [200/383] | loss: 1.2528 (avg: 0.7365) | acc: 0.5781 (avg: 0.4751) |\n",
      "Epoch 36 [300/383] | loss: 1.3652 (avg: 1.1010) | acc: 0.4922 (avg: 0.4763) |\n",
      "Validation | acc (nat): 0.7530 | acc (rob): 0.4890 |\n",
      "Epoch 37 [0/383] | loss: 1.4242 (avg: 0.0037) | acc: 0.4531 (avg: 0.4531) |\n",
      "Epoch 37 [100/383] | loss: 1.4254 (avg: 0.3673) | acc: 0.4531 (avg: 0.4790) |\n",
      "Epoch 37 [200/383] | loss: 1.4489 (avg: 0.7354) | acc: 0.4297 (avg: 0.4768) |\n",
      "Epoch 37 [300/383] | loss: 1.4737 (avg: 1.1048) | acc: 0.4688 (avg: 0.4736) |\n",
      "Validation | acc (nat): 0.7480 | acc (rob): 0.4530 |\n",
      "Epoch 38 [0/383] | loss: 1.4938 (avg: 0.0039) | acc: 0.4453 (avg: 0.4453) |\n",
      "Epoch 38 [100/383] | loss: 1.3460 (avg: 0.3694) | acc: 0.5156 (avg: 0.4708) |\n",
      "Epoch 38 [200/383] | loss: 1.2548 (avg: 0.7332) | acc: 0.4688 (avg: 0.4719) |\n",
      "Epoch 38 [300/383] | loss: 1.4558 (avg: 1.0992) | acc: 0.4453 (avg: 0.4749) |\n",
      "Validation | acc (nat): 0.7630 | acc (rob): 0.4860 |\n",
      "Epoch 39 [0/383] | loss: 1.2205 (avg: 0.0032) | acc: 0.5234 (avg: 0.5234) |\n",
      "Epoch 39 [100/383] | loss: 1.3506 (avg: 0.3698) | acc: 0.5234 (avg: 0.4743) |\n",
      "Epoch 39 [200/383] | loss: 1.3376 (avg: 0.7339) | acc: 0.4844 (avg: 0.4735) |\n",
      "Epoch 39 [300/383] | loss: 1.5175 (avg: 1.1054) | acc: 0.4531 (avg: 0.4707) |\n",
      "Validation | acc (nat): 0.7810 | acc (rob): 0.4780 |\n",
      "Epoch 40 [0/383] | loss: 1.3352 (avg: 0.0035) | acc: 0.5000 (avg: 0.5000) |\n",
      "Epoch 40 [100/383] | loss: 1.3605 (avg: 0.3647) | acc: 0.4766 (avg: 0.4805) |\n",
      "Epoch 40 [200/383] | loss: 1.3465 (avg: 0.7276) | acc: 0.5000 (avg: 0.4797) |\n",
      "Epoch 40 [300/383] | loss: 1.3723 (avg: 1.0966) | acc: 0.4766 (avg: 0.4757) |\n",
      "Validation | acc (nat): 0.7690 | acc (rob): 0.4930 |\n",
      "Epoch 41 [0/383] | loss: 1.5333 (avg: 0.0040) | acc: 0.4062 (avg: 0.4062) |\n",
      "Epoch 41 [100/383] | loss: 1.3402 (avg: 0.3663) | acc: 0.4922 (avg: 0.4797) |\n",
      "Epoch 41 [200/383] | loss: 1.4098 (avg: 0.7355) | acc: 0.4453 (avg: 0.4697) |\n",
      "Epoch 41 [300/383] | loss: 1.4536 (avg: 1.1016) | acc: 0.4219 (avg: 0.4704) |\n",
      "Validation | acc (nat): 0.7630 | acc (rob): 0.4830 |\n",
      "Epoch 42 [0/383] | loss: 1.5072 (avg: 0.0039) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 42 [100/383] | loss: 1.4164 (avg: 0.3641) | acc: 0.4688 (avg: 0.4817) |\n",
      "Epoch 42 [200/383] | loss: 1.2870 (avg: 0.7283) | acc: 0.4375 (avg: 0.4771) |\n",
      "Epoch 42 [300/383] | loss: 1.4002 (avg: 1.0956) | acc: 0.5312 (avg: 0.4755) |\n",
      "Validation | acc (nat): 0.7600 | acc (rob): 0.4760 |\n",
      "Epoch 43 [0/383] | loss: 1.2147 (avg: 0.0032) | acc: 0.5547 (avg: 0.5547) |\n",
      "Epoch 43 [100/383] | loss: 1.4436 (avg: 0.3584) | acc: 0.4531 (avg: 0.4873) |\n",
      "Epoch 43 [200/383] | loss: 1.5573 (avg: 0.7254) | acc: 0.3750 (avg: 0.4796) |\n",
      "Epoch 43 [300/383] | loss: 1.4784 (avg: 1.0908) | acc: 0.4375 (avg: 0.4777) |\n",
      "Validation | acc (nat): 0.7720 | acc (rob): 0.4840 |\n",
      "Epoch 44 [0/383] | loss: 1.4075 (avg: 0.0037) | acc: 0.4219 (avg: 0.4219) |\n",
      "Epoch 44 [100/383] | loss: 1.2811 (avg: 0.3665) | acc: 0.5547 (avg: 0.4770) |\n",
      "Epoch 44 [200/383] | loss: 1.3442 (avg: 0.7263) | acc: 0.4766 (avg: 0.4806) |\n",
      "Epoch 44 [300/383] | loss: 1.3681 (avg: 1.0961) | acc: 0.4844 (avg: 0.4778) |\n",
      "Validation | acc (nat): 0.7530 | acc (rob): 0.4800 |\n",
      "Epoch 45 [0/383] | loss: 1.3800 (avg: 0.0036) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 45 [100/383] | loss: 1.3116 (avg: 0.3658) | acc: 0.5078 (avg: 0.4735) |\n",
      "Epoch 45 [200/383] | loss: 1.3734 (avg: 0.7270) | acc: 0.4922 (avg: 0.4785) |\n",
      "Epoch 45 [300/383] | loss: 1.3098 (avg: 1.0927) | acc: 0.5078 (avg: 0.4783) |\n",
      "Validation | acc (nat): 0.7480 | acc (rob): 0.4620 |\n",
      "Epoch 46 [0/383] | loss: 1.3893 (avg: 0.0036) | acc: 0.4453 (avg: 0.4453) |\n",
      "Epoch 46 [100/383] | loss: 1.3102 (avg: 0.3598) | acc: 0.4922 (avg: 0.4859) |\n",
      "Epoch 46 [200/383] | loss: 1.2466 (avg: 0.7246) | acc: 0.5859 (avg: 0.4810) |\n",
      "Epoch 46 [300/383] | loss: 1.4858 (avg: 1.0891) | acc: 0.4141 (avg: 0.4789) |\n",
      "Validation | acc (nat): 0.7400 | acc (rob): 0.5060 |\n",
      "Epoch 47 [0/383] | loss: 1.3807 (avg: 0.0036) | acc: 0.4688 (avg: 0.4688) |\n",
      "Epoch 47 [100/383] | loss: 1.5080 (avg: 0.3635) | acc: 0.4297 (avg: 0.4838) |\n",
      "Epoch 47 [200/383] | loss: 1.3250 (avg: 0.7248) | acc: 0.5234 (avg: 0.4784) |\n",
      "Epoch 47 [300/383] | loss: 1.5219 (avg: 1.0898) | acc: 0.4688 (avg: 0.4797) |\n",
      "Validation | acc (nat): 0.7540 | acc (rob): 0.4870 |\n",
      "Epoch 48 [0/383] | loss: 1.4031 (avg: 0.0037) | acc: 0.4609 (avg: 0.4609) |\n",
      "Epoch 48 [100/383] | loss: 1.4246 (avg: 0.3615) | acc: 0.4531 (avg: 0.4831) |\n",
      "Epoch 48 [200/383] | loss: 1.3760 (avg: 0.7258) | acc: 0.5156 (avg: 0.4797) |\n",
      "Epoch 48 [300/383] | loss: 1.4823 (avg: 1.0931) | acc: 0.4375 (avg: 0.4764) |\n",
      "Validation | acc (nat): 0.7660 | acc (rob): 0.4730 |\n",
      "Epoch 49 [0/383] | loss: 1.3602 (avg: 0.0036) | acc: 0.4922 (avg: 0.4922) |\n",
      "Epoch 49 [100/383] | loss: 1.3818 (avg: 0.3606) | acc: 0.4688 (avg: 0.4828) |\n",
      "Epoch 49 [200/383] | loss: 1.4065 (avg: 0.7212) | acc: 0.4922 (avg: 0.4830) |\n",
      "Epoch 49 [300/383] | loss: 1.4211 (avg: 1.0888) | acc: 0.4609 (avg: 0.4788) |\n",
      "Validation | acc (nat): 0.7600 | acc (rob): 0.5030 |\n",
      "Epoch 50 [0/383] | loss: 1.2960 (avg: 0.0034) | acc: 0.5547 (avg: 0.5547) |\n",
      "Epoch 50 [100/383] | loss: 1.0426 (avg: 0.3213) | acc: 0.6172 (avg: 0.5452) |\n",
      "Epoch 50 [200/383] | loss: 0.9875 (avg: 0.6206) | acc: 0.6016 (avg: 0.5548) |\n",
      "Epoch 50 [300/383] | loss: 1.0844 (avg: 0.9160) | acc: 0.6016 (avg: 0.5585) |\n",
      "Validation | acc (nat): 0.8510 | acc (rob): 0.5640 |\n",
      "Epoch 51 [0/383] | loss: 1.1361 (avg: 0.0030) | acc: 0.5391 (avg: 0.5391) |\n",
      "Epoch 51 [100/383] | loss: 1.0331 (avg: 0.2832) | acc: 0.6094 (avg: 0.5908) |\n",
      "Epoch 51 [200/383] | loss: 0.9019 (avg: 0.5591) | acc: 0.6328 (avg: 0.5945) |\n",
      "Epoch 51 [300/383] | loss: 1.1143 (avg: 0.8380) | acc: 0.5547 (avg: 0.5925) |\n",
      "Validation | acc (nat): 0.8540 | acc (rob): 0.5660 |\n",
      "Epoch 52 [0/383] | loss: 1.1202 (avg: 0.0029) | acc: 0.5703 (avg: 0.5703) |\n",
      "Epoch 52 [100/383] | loss: 1.1634 (avg: 0.2704) | acc: 0.5469 (avg: 0.6063) |\n",
      "Epoch 52 [200/383] | loss: 0.9502 (avg: 0.5381) | acc: 0.6094 (avg: 0.6061) |\n",
      "Epoch 52 [300/383] | loss: 1.0539 (avg: 0.8076) | acc: 0.6094 (avg: 0.6063) |\n",
      "Validation | acc (nat): 0.8570 | acc (rob): 0.5600 |\n",
      "Epoch 53 [0/383] | loss: 0.9431 (avg: 0.0025) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 53 [100/383] | loss: 0.8718 (avg: 0.2635) | acc: 0.6797 (avg: 0.6159) |\n",
      "Epoch 53 [200/383] | loss: 0.9356 (avg: 0.5240) | acc: 0.6328 (avg: 0.6154) |\n",
      "Epoch 53 [300/383] | loss: 0.9475 (avg: 0.7857) | acc: 0.6562 (avg: 0.6125) |\n",
      "Validation | acc (nat): 0.8610 | acc (rob): 0.5870 |\n",
      "Epoch 54 [0/383] | loss: 0.9363 (avg: 0.0024) | acc: 0.6172 (avg: 0.6172) |\n",
      "Epoch 54 [100/383] | loss: 0.9905 (avg: 0.2532) | acc: 0.6562 (avg: 0.6238) |\n",
      "Epoch 54 [200/383] | loss: 1.0742 (avg: 0.5054) | acc: 0.5625 (avg: 0.6265) |\n",
      "Epoch 54 [300/383] | loss: 0.9493 (avg: 0.7617) | acc: 0.6250 (avg: 0.6237) |\n",
      "Validation | acc (nat): 0.8550 | acc (rob): 0.5770 |\n",
      "Epoch 55 [0/383] | loss: 0.9206 (avg: 0.0024) | acc: 0.6484 (avg: 0.6484) |\n",
      "Epoch 55 [100/383] | loss: 0.8496 (avg: 0.2463) | acc: 0.6641 (avg: 0.6386) |\n",
      "Epoch 55 [200/383] | loss: 1.1612 (avg: 0.4984) | acc: 0.5234 (avg: 0.6318) |\n",
      "Epoch 55 [300/383] | loss: 1.0034 (avg: 0.7512) | acc: 0.6328 (avg: 0.6299) |\n",
      "Validation | acc (nat): 0.8690 | acc (rob): 0.5660 |\n",
      "Epoch 56 [0/383] | loss: 0.9518 (avg: 0.0025) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 56 [100/383] | loss: 0.9820 (avg: 0.2421) | acc: 0.6250 (avg: 0.6409) |\n",
      "Epoch 56 [200/383] | loss: 0.9284 (avg: 0.4858) | acc: 0.6641 (avg: 0.6381) |\n",
      "Epoch 56 [300/383] | loss: 0.9145 (avg: 0.7288) | acc: 0.6484 (avg: 0.6381) |\n",
      "Validation | acc (nat): 0.8640 | acc (rob): 0.5610 |\n",
      "Epoch 57 [0/383] | loss: 0.9038 (avg: 0.0024) | acc: 0.6172 (avg: 0.6172) |\n",
      "Epoch 57 [100/383] | loss: 0.8696 (avg: 0.2346) | acc: 0.6875 (avg: 0.6549) |\n",
      "Epoch 57 [200/383] | loss: 0.7646 (avg: 0.4744) | acc: 0.7422 (avg: 0.6458) |\n",
      "Epoch 57 [300/383] | loss: 1.0091 (avg: 0.7139) | acc: 0.6016 (avg: 0.6442) |\n",
      "Validation | acc (nat): 0.8650 | acc (rob): 0.5630 |\n",
      "Epoch 58 [0/383] | loss: 0.9549 (avg: 0.0025) | acc: 0.6250 (avg: 0.6250) |\n",
      "Epoch 58 [100/383] | loss: 0.6995 (avg: 0.2311) | acc: 0.7656 (avg: 0.6586) |\n",
      "Epoch 58 [200/383] | loss: 1.0226 (avg: 0.4657) | acc: 0.5938 (avg: 0.6514) |\n",
      "Epoch 58 [300/383] | loss: 0.8432 (avg: 0.7006) | acc: 0.6406 (avg: 0.6501) |\n",
      "Validation | acc (nat): 0.8610 | acc (rob): 0.5590 |\n",
      "Epoch 59 [0/383] | loss: 0.7873 (avg: 0.0021) | acc: 0.7344 (avg: 0.7344) |\n",
      "Epoch 59 [100/383] | loss: 0.8881 (avg: 0.2259) | acc: 0.6406 (avg: 0.6674) |\n",
      "Epoch 59 [200/383] | loss: 0.8199 (avg: 0.4577) | acc: 0.7266 (avg: 0.6595) |\n",
      "Epoch 59 [300/383] | loss: 1.0014 (avg: 0.6889) | acc: 0.6094 (avg: 0.6576) |\n",
      "Validation | acc (nat): 0.8580 | acc (rob): 0.5480 |\n",
      "Epoch 60 [0/383] | loss: 0.9566 (avg: 0.0025) | acc: 0.6406 (avg: 0.6406) |\n",
      "Epoch 60 [100/383] | loss: 0.9869 (avg: 0.2225) | acc: 0.6250 (avg: 0.6700) |\n",
      "Epoch 60 [200/383] | loss: 0.7281 (avg: 0.4444) | acc: 0.6719 (avg: 0.6666) |\n",
      "Epoch 60 [300/383] | loss: 0.8746 (avg: 0.6775) | acc: 0.7266 (avg: 0.6598) |\n",
      "Validation | acc (nat): 0.8470 | acc (rob): 0.5610 |\n",
      "Epoch 61 [0/383] | loss: 0.7936 (avg: 0.0021) | acc: 0.6641 (avg: 0.6641) |\n",
      "Epoch 61 [100/383] | loss: 0.9067 (avg: 0.2199) | acc: 0.6406 (avg: 0.6706) |\n",
      "Epoch 61 [200/383] | loss: 0.9839 (avg: 0.4402) | acc: 0.5703 (avg: 0.6668) |\n",
      "Epoch 61 [300/383] | loss: 1.0588 (avg: 0.6674) | acc: 0.5703 (avg: 0.6632) |\n",
      "Validation | acc (nat): 0.8630 | acc (rob): 0.5620 |\n",
      "Epoch 62 [0/383] | loss: 0.8154 (avg: 0.0021) | acc: 0.6875 (avg: 0.6875) |\n",
      "Epoch 62 [100/383] | loss: 0.7170 (avg: 0.2167) | acc: 0.6953 (avg: 0.6791) |\n",
      "Epoch 62 [200/383] | loss: 0.8632 (avg: 0.4341) | acc: 0.6953 (avg: 0.6738) |\n",
      "Epoch 62 [300/383] | loss: 0.8484 (avg: 0.6571) | acc: 0.6484 (avg: 0.6696) |\n",
      "Validation | acc (nat): 0.8520 | acc (rob): 0.5600 |\n",
      "Epoch 63 [0/383] | loss: 0.8861 (avg: 0.0023) | acc: 0.6406 (avg: 0.6406) |\n",
      "Epoch 63 [100/383] | loss: 0.8236 (avg: 0.2116) | acc: 0.6719 (avg: 0.6790) |\n",
      "Epoch 63 [200/383] | loss: 0.6817 (avg: 0.4289) | acc: 0.7266 (avg: 0.6740) |\n",
      "Epoch 63 [300/383] | loss: 0.7876 (avg: 0.6515) | acc: 0.6641 (avg: 0.6689) |\n",
      "Validation | acc (nat): 0.8590 | acc (rob): 0.5630 |\n",
      "Epoch 64 [0/383] | loss: 0.8229 (avg: 0.0021) | acc: 0.6562 (avg: 0.6562) |\n",
      "Epoch 64 [100/383] | loss: 0.7933 (avg: 0.2075) | acc: 0.6562 (avg: 0.6796) |\n",
      "Epoch 64 [200/383] | loss: 0.8541 (avg: 0.4184) | acc: 0.6641 (avg: 0.6775) |\n",
      "Epoch 64 [300/383] | loss: 0.7914 (avg: 0.6401) | acc: 0.7031 (avg: 0.6731) |\n",
      "Validation | acc (nat): 0.8740 | acc (rob): 0.5420 |\n",
      "Epoch 65 [0/383] | loss: 0.7295 (avg: 0.0019) | acc: 0.7109 (avg: 0.7109) |\n",
      "Epoch 65 [100/383] | loss: 0.9659 (avg: 0.2084) | acc: 0.6406 (avg: 0.6843) |\n",
      "Epoch 65 [200/383] | loss: 0.8583 (avg: 0.4237) | acc: 0.6641 (avg: 0.6784) |\n",
      "Epoch 65 [300/383] | loss: 0.7615 (avg: 0.6395) | acc: 0.6797 (avg: 0.6754) |\n",
      "Validation | acc (nat): 0.8600 | acc (rob): 0.5580 |\n",
      "Epoch 66 [0/383] | loss: 0.7956 (avg: 0.0021) | acc: 0.7500 (avg: 0.7500) |\n",
      "Epoch 66 [100/383] | loss: 0.7089 (avg: 0.2003) | acc: 0.6953 (avg: 0.6961) |\n",
      "Epoch 66 [200/383] | loss: 0.7018 (avg: 0.4071) | acc: 0.7188 (avg: 0.6891) |\n",
      "Epoch 66 [300/383] | loss: 0.7861 (avg: 0.6187) | acc: 0.6797 (avg: 0.6847) |\n",
      "Validation | acc (nat): 0.8490 | acc (rob): 0.5510 |\n",
      "Epoch 67 [0/383] | loss: 0.7094 (avg: 0.0019) | acc: 0.7500 (avg: 0.7500) |\n",
      "Epoch 67 [100/383] | loss: 0.7060 (avg: 0.2002) | acc: 0.6953 (avg: 0.6894) |\n",
      "Epoch 67 [200/383] | loss: 0.7669 (avg: 0.4042) | acc: 0.6797 (avg: 0.6882) |\n",
      "Epoch 67 [300/383] | loss: 0.9501 (avg: 0.6139) | acc: 0.6172 (avg: 0.6846) |\n",
      "Validation | acc (nat): 0.8510 | acc (rob): 0.5260 |\n",
      "Epoch 68 [0/383] | loss: 0.6689 (avg: 0.0017) | acc: 0.7188 (avg: 0.7188) |\n",
      "Epoch 68 [100/383] | loss: 0.7579 (avg: 0.1973) | acc: 0.6875 (avg: 0.7020) |\n",
      "Epoch 68 [200/383] | loss: 0.8257 (avg: 0.3981) | acc: 0.5781 (avg: 0.6945) |\n",
      "Epoch 68 [300/383] | loss: 0.7682 (avg: 0.6039) | acc: 0.7188 (avg: 0.6897) |\n",
      "Validation | acc (nat): 0.8740 | acc (rob): 0.5390 |\n",
      "Epoch 69 [0/383] | loss: 0.7448 (avg: 0.0019) | acc: 0.7266 (avg: 0.7266) |\n",
      "Epoch 69 [100/383] | loss: 0.7322 (avg: 0.1952) | acc: 0.6953 (avg: 0.7017) |\n",
      "Epoch 69 [200/383] | loss: 0.7825 (avg: 0.3945) | acc: 0.7266 (avg: 0.6968) |\n",
      "Epoch 69 [300/383] | loss: 0.7564 (avg: 0.6006) | acc: 0.6484 (avg: 0.6923) |\n",
      "Validation | acc (nat): 0.8750 | acc (rob): 0.5460 |\n",
      "Epoch 70 [0/383] | loss: 0.6913 (avg: 0.0018) | acc: 0.7031 (avg: 0.7031) |\n",
      "Epoch 70 [100/383] | loss: 0.7619 (avg: 0.1915) | acc: 0.7031 (avg: 0.7070) |\n",
      "Epoch 70 [200/383] | loss: 0.6785 (avg: 0.3872) | acc: 0.7344 (avg: 0.7027) |\n",
      "Epoch 70 [300/383] | loss: 0.8835 (avg: 0.5878) | acc: 0.6250 (avg: 0.6993) |\n",
      "Validation | acc (nat): 0.8500 | acc (rob): 0.5550 |\n",
      "Epoch 71 [0/383] | loss: 0.6464 (avg: 0.0017) | acc: 0.7812 (avg: 0.7812) |\n",
      "Epoch 71 [100/383] | loss: 0.7138 (avg: 0.1879) | acc: 0.6641 (avg: 0.7142) |\n",
      "Epoch 71 [200/383] | loss: 0.8507 (avg: 0.3819) | acc: 0.6406 (avg: 0.7067) |\n",
      "Epoch 71 [300/383] | loss: 0.6862 (avg: 0.5816) | acc: 0.7734 (avg: 0.7006) |\n",
      "Validation | acc (nat): 0.8450 | acc (rob): 0.5400 |\n",
      "Epoch 72 [0/383] | loss: 0.7538 (avg: 0.0020) | acc: 0.7031 (avg: 0.7031) |\n",
      "Epoch 72 [100/383] | loss: 0.7134 (avg: 0.1863) | acc: 0.7109 (avg: 0.7160) |\n",
      "Epoch 72 [200/383] | loss: 0.7518 (avg: 0.3776) | acc: 0.7031 (avg: 0.7073) |\n",
      "Epoch 72 [300/383] | loss: 0.8180 (avg: 0.5773) | acc: 0.6719 (avg: 0.7020) |\n",
      "Validation | acc (nat): 0.8650 | acc (rob): 0.5320 |\n",
      "Epoch 73 [0/383] | loss: 0.6634 (avg: 0.0017) | acc: 0.7422 (avg: 0.7422) |\n",
      "Epoch 73 [100/383] | loss: 0.7320 (avg: 0.1834) | acc: 0.7344 (avg: 0.7157) |\n",
      "Epoch 73 [200/383] | loss: 0.5822 (avg: 0.3735) | acc: 0.7812 (avg: 0.7107) |\n",
      "Epoch 73 [300/383] | loss: 0.8985 (avg: 0.5674) | acc: 0.6406 (avg: 0.7058) |\n",
      "Validation | acc (nat): 0.8570 | acc (rob): 0.5360 |\n",
      "Epoch 74 [0/383] | loss: 0.7914 (avg: 0.0021) | acc: 0.6641 (avg: 0.6641) |\n",
      "Epoch 74 [100/383] | loss: 0.8899 (avg: 0.1809) | acc: 0.6328 (avg: 0.7211) |\n",
      "Epoch 74 [200/383] | loss: 0.7264 (avg: 0.3671) | acc: 0.6797 (avg: 0.7132) |\n",
      "Epoch 74 [300/383] | loss: 0.8508 (avg: 0.5602) | acc: 0.6953 (avg: 0.7090) |\n",
      "Validation | acc (nat): 0.8700 | acc (rob): 0.5500 |\n",
      "Epoch 75 [0/383] | loss: 0.6635 (avg: 0.0017) | acc: 0.7031 (avg: 0.7031) |\n",
      "Epoch 75 [100/383] | loss: 0.6263 (avg: 0.1595) | acc: 0.6875 (avg: 0.7536) |\n",
      "Epoch 75 [200/383] | loss: 0.5580 (avg: 0.3004) | acc: 0.7500 (avg: 0.7680) |\n",
      "Epoch 75 [300/383] | loss: 0.4450 (avg: 0.4359) | acc: 0.8125 (avg: 0.7743) |\n",
      "Validation | acc (nat): 0.8830 | acc (rob): 0.5460 |\n",
      "Epoch 76 [0/383] | loss: 0.5275 (avg: 0.0014) | acc: 0.7891 (avg: 0.7891) |\n",
      "Epoch 76 [100/383] | loss: 0.5214 (avg: 0.1278) | acc: 0.7891 (avg: 0.8038) |\n",
      "Epoch 76 [200/383] | loss: 0.3866 (avg: 0.2510) | acc: 0.8828 (avg: 0.8066) |\n",
      "Epoch 76 [300/383] | loss: 0.4866 (avg: 0.3743) | acc: 0.8125 (avg: 0.8071) |\n",
      "Validation | acc (nat): 0.8840 | acc (rob): 0.5350 |\n",
      "Epoch 77 [0/383] | loss: 0.4579 (avg: 0.0012) | acc: 0.7812 (avg: 0.7812) |\n",
      "Epoch 77 [100/383] | loss: 0.3988 (avg: 0.1172) | acc: 0.8125 (avg: 0.8172) |\n",
      "Epoch 77 [200/383] | loss: 0.3969 (avg: 0.2356) | acc: 0.7969 (avg: 0.8130) |\n",
      "Epoch 77 [300/383] | loss: 0.3867 (avg: 0.3509) | acc: 0.8203 (avg: 0.8156) |\n",
      "Validation | acc (nat): 0.8940 | acc (rob): 0.5560 |\n",
      "Epoch 78 [0/383] | loss: 0.4233 (avg: 0.0011) | acc: 0.8281 (avg: 0.8281) |\n",
      "Epoch 78 [100/383] | loss: 0.3278 (avg: 0.1092) | acc: 0.8984 (avg: 0.8317) |\n",
      "Epoch 78 [200/383] | loss: 0.3846 (avg: 0.2194) | acc: 0.8125 (avg: 0.8287) |\n",
      "Epoch 78 [300/383] | loss: 0.4035 (avg: 0.3308) | acc: 0.8047 (avg: 0.8273) |\n",
      "Validation | acc (nat): 0.8800 | acc (rob): 0.5560 |\n",
      "Epoch 79 [0/383] | loss: 0.4087 (avg: 0.0011) | acc: 0.8359 (avg: 0.8359) |\n",
      "Epoch 79 [100/383] | loss: 0.3407 (avg: 0.1045) | acc: 0.8750 (avg: 0.8391) |\n",
      "Epoch 79 [200/383] | loss: 0.4341 (avg: 0.2104) | acc: 0.8125 (avg: 0.8352) |\n",
      "Epoch 79 [300/383] | loss: 0.4134 (avg: 0.3174) | acc: 0.8203 (avg: 0.8341) |\n",
      "Validation | acc (nat): 0.8780 | acc (rob): 0.5440 |\n",
      "Epoch 80 [0/383] | loss: 0.3866 (avg: 0.0010) | acc: 0.8594 (avg: 0.8594) |\n",
      "Epoch 80 [100/383] | loss: 0.3191 (avg: 0.1029) | acc: 0.8984 (avg: 0.8408) |\n",
      "Epoch 80 [200/383] | loss: 0.3357 (avg: 0.2036) | acc: 0.8359 (avg: 0.8413) |\n",
      "Epoch 80 [300/383] | loss: 0.4651 (avg: 0.3041) | acc: 0.8203 (avg: 0.8407) |\n",
      "Validation | acc (nat): 0.8880 | acc (rob): 0.5340 |\n",
      "Epoch 81 [0/383] | loss: 0.4191 (avg: 0.0011) | acc: 0.8047 (avg: 0.8047) |\n",
      "Epoch 81 [100/383] | loss: 0.3733 (avg: 0.0992) | acc: 0.8359 (avg: 0.8407) |\n",
      "Epoch 81 [200/383] | loss: 0.2870 (avg: 0.1941) | acc: 0.8750 (avg: 0.8449) |\n",
      "Epoch 81 [300/383] | loss: 0.4390 (avg: 0.2942) | acc: 0.8203 (avg: 0.8439) |\n",
      "Validation | acc (nat): 0.8790 | acc (rob): 0.5420 |\n",
      "Epoch 82 [0/383] | loss: 0.3960 (avg: 0.0010) | acc: 0.8047 (avg: 0.8047) |\n",
      "Epoch 82 [100/383] | loss: 0.3449 (avg: 0.0958) | acc: 0.8672 (avg: 0.8500) |\n",
      "Epoch 82 [200/383] | loss: 0.4140 (avg: 0.1895) | acc: 0.8203 (avg: 0.8521) |\n",
      "Epoch 82 [300/383] | loss: 0.3840 (avg: 0.2845) | acc: 0.8359 (avg: 0.8512) |\n",
      "Validation | acc (nat): 0.8850 | acc (rob): 0.5270 |\n",
      "Epoch 83 [0/383] | loss: 0.3121 (avg: 0.0008) | acc: 0.8984 (avg: 0.8984) |\n",
      "Epoch 83 [100/383] | loss: 0.3127 (avg: 0.0894) | acc: 0.8906 (avg: 0.8615) |\n",
      "Epoch 83 [200/383] | loss: 0.3286 (avg: 0.1811) | acc: 0.8672 (avg: 0.8598) |\n",
      "Epoch 83 [300/383] | loss: 0.3402 (avg: 0.2737) | acc: 0.8828 (avg: 0.8585) |\n",
      "Validation | acc (nat): 0.8840 | acc (rob): 0.5300 |\n",
      "Epoch 84 [0/383] | loss: 0.2786 (avg: 0.0007) | acc: 0.8750 (avg: 0.8750) |\n",
      "Epoch 84 [100/383] | loss: 0.4443 (avg: 0.0888) | acc: 0.7969 (avg: 0.8595) |\n",
      "Epoch 84 [200/383] | loss: 0.3602 (avg: 0.1762) | acc: 0.8438 (avg: 0.8600) |\n",
      "Epoch 84 [300/383] | loss: 0.3648 (avg: 0.2635) | acc: 0.8438 (avg: 0.8600) |\n",
      "Validation | acc (nat): 0.8760 | acc (rob): 0.5340 |\n",
      "Epoch 85 [0/383] | loss: 0.3420 (avg: 0.0009) | acc: 0.8359 (avg: 0.8359) |\n",
      "Epoch 85 [100/383] | loss: 0.3422 (avg: 0.0867) | acc: 0.8594 (avg: 0.8659) |\n",
      "Epoch 85 [200/383] | loss: 0.2974 (avg: 0.1725) | acc: 0.8594 (avg: 0.8644) |\n",
      "Epoch 85 [300/383] | loss: 0.2921 (avg: 0.2585) | acc: 0.8672 (avg: 0.8640) |\n",
      "Validation | acc (nat): 0.8740 | acc (rob): 0.5350 |\n",
      "Epoch 86 [0/383] | loss: 0.3147 (avg: 0.0008) | acc: 0.8906 (avg: 0.8906) |\n",
      "Epoch 86 [100/383] | loss: 0.2886 (avg: 0.0828) | acc: 0.8906 (avg: 0.8687) |\n",
      "Epoch 86 [200/383] | loss: 0.3712 (avg: 0.1668) | acc: 0.8281 (avg: 0.8670) |\n",
      "Epoch 86 [300/383] | loss: 0.3699 (avg: 0.2508) | acc: 0.8828 (avg: 0.8670) |\n",
      "Validation | acc (nat): 0.8770 | acc (rob): 0.5410 |\n",
      "Epoch 87 [0/383] | loss: 0.3431 (avg: 0.0009) | acc: 0.8594 (avg: 0.8594) |\n",
      "Epoch 87 [100/383] | loss: 0.2982 (avg: 0.0810) | acc: 0.8828 (avg: 0.8745) |\n",
      "Epoch 87 [200/383] | loss: 0.2869 (avg: 0.1601) | acc: 0.8516 (avg: 0.8733) |\n",
      "Epoch 87 [300/383] | loss: 0.3301 (avg: 0.2408) | acc: 0.8281 (avg: 0.8729) |\n",
      "Validation | acc (nat): 0.8810 | acc (rob): 0.5330 |\n",
      "Epoch 88 [0/383] | loss: 0.3040 (avg: 0.0008) | acc: 0.8672 (avg: 0.8672) |\n",
      "Epoch 88 [100/383] | loss: 0.3440 (avg: 0.0781) | acc: 0.8359 (avg: 0.8780) |\n",
      "Epoch 88 [200/383] | loss: 0.3061 (avg: 0.1544) | acc: 0.8828 (avg: 0.8797) |\n",
      "Epoch 88 [300/383] | loss: 0.3507 (avg: 0.2336) | acc: 0.8516 (avg: 0.8775) |\n",
      "Validation | acc (nat): 0.8690 | acc (rob): 0.5340 |\n",
      "Epoch 89 [0/383] | loss: 0.2663 (avg: 0.0007) | acc: 0.8906 (avg: 0.8906) |\n",
      "Epoch 89 [100/383] | loss: 0.3498 (avg: 0.0765) | acc: 0.8672 (avg: 0.8825) |\n",
      "Epoch 89 [200/383] | loss: 0.2830 (avg: 0.1525) | acc: 0.8672 (avg: 0.8818) |\n",
      "Epoch 89 [300/383] | loss: 0.3590 (avg: 0.2308) | acc: 0.8672 (avg: 0.8794) |\n",
      "Validation | acc (nat): 0.8830 | acc (rob): 0.5360 |\n",
      "Epoch 90 [0/383] | loss: 0.2987 (avg: 0.0008) | acc: 0.8828 (avg: 0.8828) |\n",
      "Epoch 90 [100/383] | loss: 0.2487 (avg: 0.0732) | acc: 0.9062 (avg: 0.8883) |\n",
      "Epoch 90 [200/383] | loss: 0.3800 (avg: 0.1469) | acc: 0.8438 (avg: 0.8857) |\n",
      "Epoch 90 [300/383] | loss: 0.3281 (avg: 0.2232) | acc: 0.8984 (avg: 0.8848) |\n",
      "Validation | acc (nat): 0.8810 | acc (rob): 0.5270 |\n",
      "Epoch 91 [0/383] | loss: 0.2486 (avg: 0.0006) | acc: 0.8906 (avg: 0.8906) |\n",
      "Epoch 91 [100/383] | loss: 0.4229 (avg: 0.0717) | acc: 0.8359 (avg: 0.8876) |\n",
      "Epoch 91 [200/383] | loss: 0.1640 (avg: 0.1447) | acc: 0.9219 (avg: 0.8855) |\n",
      "Epoch 91 [300/383] | loss: 0.3201 (avg: 0.2177) | acc: 0.8828 (avg: 0.8848) |\n",
      "Validation | acc (nat): 0.8630 | acc (rob): 0.5220 |\n",
      "Epoch 92 [0/383] | loss: 0.3095 (avg: 0.0008) | acc: 0.8828 (avg: 0.8828) |\n",
      "Epoch 92 [100/383] | loss: 0.1901 (avg: 0.0687) | acc: 0.9297 (avg: 0.8943) |\n",
      "Epoch 92 [200/383] | loss: 0.2482 (avg: 0.1395) | acc: 0.8828 (avg: 0.8888) |\n",
      "Epoch 92 [300/383] | loss: 0.2795 (avg: 0.2116) | acc: 0.8906 (avg: 0.8876) |\n",
      "Validation | acc (nat): 0.8600 | acc (rob): 0.5270 |\n",
      "Epoch 93 [0/383] | loss: 0.2810 (avg: 0.0007) | acc: 0.8906 (avg: 0.8906) |\n",
      "Epoch 93 [100/383] | loss: 0.2463 (avg: 0.0678) | acc: 0.8984 (avg: 0.8909) |\n",
      "Epoch 93 [200/383] | loss: 0.2662 (avg: 0.1368) | acc: 0.8984 (avg: 0.8891) |\n",
      "Epoch 93 [300/383] | loss: 0.2099 (avg: 0.2062) | acc: 0.9141 (avg: 0.8891) |\n",
      "Validation | acc (nat): 0.8810 | acc (rob): 0.5170 |\n",
      "Epoch 94 [0/383] | loss: 0.3012 (avg: 0.0008) | acc: 0.8750 (avg: 0.8750) |\n",
      "Epoch 94 [100/383] | loss: 0.2635 (avg: 0.0647) | acc: 0.8750 (avg: 0.8989) |\n",
      "Epoch 94 [200/383] | loss: 0.2475 (avg: 0.1309) | acc: 0.9141 (avg: 0.8958) |\n",
      "Epoch 94 [300/383] | loss: 0.2438 (avg: 0.1982) | acc: 0.8984 (avg: 0.8948) |\n",
      "Validation | acc (nat): 0.8710 | acc (rob): 0.5260 |\n",
      "Epoch 95 [0/383] | loss: 0.2512 (avg: 0.0007) | acc: 0.8984 (avg: 0.8984) |\n",
      "Epoch 95 [100/383] | loss: 0.2648 (avg: 0.0645) | acc: 0.8906 (avg: 0.9004) |\n",
      "Epoch 95 [200/383] | loss: 0.2180 (avg: 0.1309) | acc: 0.8984 (avg: 0.8977) |\n",
      "Epoch 95 [300/383] | loss: 0.2160 (avg: 0.1973) | acc: 0.9375 (avg: 0.8964) |\n",
      "Validation | acc (nat): 0.8780 | acc (rob): 0.5430 |\n",
      "Epoch 96 [0/383] | loss: 0.2822 (avg: 0.0007) | acc: 0.8906 (avg: 0.8906) |\n",
      "Epoch 96 [100/383] | loss: 0.2708 (avg: 0.0640) | acc: 0.8828 (avg: 0.9032) |\n",
      "Epoch 96 [200/383] | loss: 0.2562 (avg: 0.1269) | acc: 0.8984 (avg: 0.9007) |\n",
      "Epoch 96 [300/383] | loss: 0.3597 (avg: 0.1922) | acc: 0.8438 (avg: 0.8992) |\n",
      "Validation | acc (nat): 0.8730 | acc (rob): 0.5270 |\n",
      "Epoch 97 [0/383] | loss: 0.1577 (avg: 0.0004) | acc: 0.9531 (avg: 0.9531) |\n",
      "Epoch 97 [100/383] | loss: 0.2908 (avg: 0.0599) | acc: 0.8906 (avg: 0.9090) |\n",
      "Epoch 97 [200/383] | loss: 0.2344 (avg: 0.1215) | acc: 0.9141 (avg: 0.9063) |\n",
      "Epoch 97 [300/383] | loss: 0.2832 (avg: 0.1834) | acc: 0.8672 (avg: 0.9050) |\n",
      "Validation | acc (nat): 0.8720 | acc (rob): 0.5220 |\n",
      "Epoch 98 [0/383] | loss: 0.2494 (avg: 0.0007) | acc: 0.9062 (avg: 0.9062) |\n",
      "Epoch 98 [100/383] | loss: 0.2507 (avg: 0.0596) | acc: 0.8906 (avg: 0.9058) |\n",
      "Epoch 98 [200/383] | loss: 0.2924 (avg: 0.1204) | acc: 0.9219 (avg: 0.9060) |\n",
      "Epoch 98 [300/383] | loss: 0.2087 (avg: 0.1828) | acc: 0.9219 (avg: 0.9036) |\n",
      "Validation | acc (nat): 0.8820 | acc (rob): 0.5260 |\n",
      "Epoch 99 [0/383] | loss: 0.1891 (avg: 0.0005) | acc: 0.9297 (avg: 0.9297) |\n",
      "Epoch 99 [100/383] | loss: 0.2422 (avg: 0.0582) | acc: 0.9062 (avg: 0.9100) |\n",
      "Epoch 99 [200/383] | loss: 0.2495 (avg: 0.1187) | acc: 0.8984 (avg: 0.9070) |\n",
      "Epoch 99 [300/383] | loss: 0.2647 (avg: 0.1779) | acc: 0.8828 (avg: 0.9068) |\n",
      "Validation | acc (nat): 0.8680 | acc (rob): 0.5070 |\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()])\n",
    "train_dataset, _ = get_dataloader(dataset, batch_size)\n",
    "num_samples = len(train_dataset)\n",
    "num_samples_for_train = int(num_samples * 0.98)\n",
    "num_samples_for_valid = num_samples - num_samples_for_train\n",
    "train_set, valid_set = random_split(train_dataset, [num_samples_for_train, num_samples_for_valid])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "model = nn.DataParallel(get_network(model_type, num_classes).cuda())\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = [int(total_epochs*0.5), int(total_epochs*0.75)]\n",
    "adjust_learning_rate = lr_scheduler.MultiStepLR(optimizer, scheduler, gamma=0.1)\n",
    "best_acc_nat, best_acc_rob = 0, 0\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    training(epoch, model, train_dataloader, optimizer, num_classes, \n",
    "             gamma, beta, pm_type, warm_up, epsilon, alpha, num_repeats)\n",
    "    test_acc_nat, test_acc_rob = evaluation(epoch, model, valid_dataloader, alpha, epsilon, num_repeats)\n",
    "        \n",
    "    is_best = best_acc_nat < test_acc_nat and best_acc_rob < test_acc_rob\n",
    "    best_acc_nat = max(best_acc_nat, test_acc_nat)\n",
    "    best_acc_rob = max(best_acc_rob, test_acc_rob)\n",
    "    save_checkpoint = {'state_dict': model.state_dict(),\n",
    "                       'best_acc_nat': best_acc_nat,\n",
    "                       'best_acc_rob': best_acc_rob,\n",
    "                       'optimizer': optimizer.state_dict(),\n",
    "                       'model_type': model_type,\n",
    "                       'dataset': dataset}\n",
    "    torch.save(save_checkpoint, os.path.join(checkpoint, 'model'))\n",
    "    if is_best:\n",
    "        torch.save(save_checkpoint, os.path.join(checkpoint, 'best_model'))\n",
    "    adjust_learning_rate.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67273057-ddb6-4294-b7ae-3aa932ab0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
