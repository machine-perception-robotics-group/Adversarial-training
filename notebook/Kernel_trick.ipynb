{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be933234-4b51-41a5-a5a0-9b831b916f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CNN with Radial Basis Function (RBF) kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85eb3db2-6a80-4e98-8a62-4fd714fc9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import yaml\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66240c81-1be4-4a34-af22-21a6292e0ff9",
   "metadata": {},
   "source": [
    "## Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de849e-a804-4062-aef7-4cfcd5999b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '4'\n",
    "dataset = 'mnist'\n",
    "model_type = 'rbf-cnn'\n",
    "checkpoint = './checkpoint/rbf_cnn/%s/%s' % (dataset, model_type)\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "total_epochs = 12\n",
    "epsilon = 80/255\n",
    "alpha = 20/255\n",
    "num_repeats = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d6ea5-a7ce-4b5e-88ca-4fb853dccf53",
   "metadata": {},
   "source": [
    "## RBF kernel module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75049c2-34c4-4fa9-abc6-246ae9091db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF(nn.Module):\n",
    "    def __init__(self, num_features, betas=2.0, use_gpu=True):\n",
    "        super(RBF, self).__init__()\n",
    "        if use_gpu:\n",
    "            self.betas = nn.Parameter(torch.randn(num_features).cuda())\n",
    "            self.center = nn.Parameter(torch.randn(num_features, num_features).cuda())\n",
    "            self.A = nn.Parameter(betas*torch.eye(num_features, num_features).cuda())\n",
    "        \n",
    "            self.weight = nn.Parameter(torch.randn(num_features, num_features).cuda())\n",
    "            self.bias = nn.Parameter(torch.randn(num_features).cuda())\n",
    "        else:\n",
    "            self.betas = nn.Parameter(torch.randn(num_features))\n",
    "            self.center = nn.Parameter(torch.randn(num_features, num_features))\n",
    "            self.A = nn.Parameter(betas*torch.eye(num_features, num_features))\n",
    "        \n",
    "            self.weight = nn.Parameter(torch.randn(num_features, num_features))\n",
    "            self.bias = nn.Parameter(torch.randn(num_features))\n",
    "\n",
    "        ## Parameter initialization\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.constant_(self.betas, val=2.0)\n",
    "        nn.init.uniform_(self.center, a=0.0, b=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        expanded_center = self.center[None,None,:,:]\n",
    "        A = self.A + sys.float_info.epsilon\n",
    "        psi = A.t() * A\n",
    "        s = x[:,:,:,None].repeat(1,1,1,x.size(2)) - expanded_center\n",
    "        dist = torch.sqrt(torch.sum(torch.tensordot(s, psi, dims=1) * s, dim=-1))\n",
    "        mahalanobis = torch.exp(-self.betas * dist)\n",
    "        \n",
    "        out = torch.tensordot(mahalanobis, self.weight, dims=1)\n",
    "        g = out + self.bias\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc2997-69ca-4f45-93d9-5a7d8bbd40c8",
   "metadata": {},
   "source": [
    "## Main network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ff3ace-9253-47e4-b55f-7259e5fe5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_trick(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Kernel_trick, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=8, stride=2, padding=3)\n",
    "        size = 14*14\n",
    "        self.rbf1 = RBF(size)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16*2, 32, kernel_size=6, stride=2)\n",
    "        size = 5*5\n",
    "        self.rbf2 = RBF(size)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32*2, 32, kernel_size=5, stride=1)\n",
    "        size = 1*1\n",
    "        self.rbf3 = RBF(size)\n",
    "        \n",
    "        self.classifier = nn.Linear(32*2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        h = self.relu(self.conv1(x))\n",
    "        g = self.rbf1(h.flatten(start_dim=2)).view(b,-1,h.size(2),h.size(3))\n",
    "        \n",
    "        h_cat = torch.cat((h, g), dim=1)\n",
    "        h = self.relu(self.conv2(h_cat))\n",
    "        g = self.rbf2(h.flatten(start_dim=2)).view(b,-1,h.size(2),h.size(3))\n",
    "        \n",
    "        h_cat = torch.cat((h, g), dim=1)\n",
    "        h = self.relu(self.conv3(h_cat))\n",
    "        g = self.rbf3(h.flatten(start_dim=2)).view(b,-1,h.size(2),h.size(3))\n",
    "        \n",
    "        h_cat = torch.cat((h, g), dim=1)\n",
    "        out = self.classifier(h_cat.flatten(start_dim=1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2591b-4ebf-4714-a3f4-85cdac7f272d",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2276c9-d52b-4544-8448-42817161084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, optimizer, num_classes):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "        \n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        batch = inputs.size(0)\n",
    "        logits = model(inputs)\n",
    "        loss = xent(logits, targets)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total += batch\n",
    "        total_loss += loss.item()\n",
    "        num_correct = torch.argmax(logits.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct += num_correct\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Epoch %d [%d/%d] | loss: %.4f (avg: %.4f) | acc: %.4f (avg: %.4f) |'\\\n",
    "                  % (epoch, idx, len(dataloader), loss.item(), total_loss/len(dataloader),\n",
    "                     num_correct/batch, total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff1c8d2-8e9c-441c-b471-9eb237eb3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch, model, dataloader, alpha, epsilon, num_repeats):\n",
    "    model.eval()\n",
    "    total_correct_nat = 0\n",
    "    total_correct_adv = 0\n",
    "    \n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    for samples in dataloader:\n",
    "        inputs, targets = samples[0].cuda(), samples[1].cuda()\n",
    "        batch = inputs.size(0)\n",
    "        with torch.enable_grad():\n",
    "            noise = torch.cuda.FloatTensor(inputs.shape).uniform_(-epsilon, epsilon)\n",
    "            x = torch.clamp(inputs+noise, min=0, max=1)\n",
    "            \n",
    "            for _ in range(num_repeats):\n",
    "                x.requires_grad_()\n",
    "                logits = model(x)\n",
    "                loss = xent(logits, targets)\n",
    "                loss.backward()\n",
    "                grads = x.grad.data\n",
    "                x = x.detach() + alpha*torch.sign(grads).detach()\n",
    "                x = torch.min(torch.max(x - epsilon), x + epsilon).clamp(min=0, max=1)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits_nat = model(inputs)\n",
    "            logits_adv = model(x)\n",
    "        \n",
    "        total_correct_nat += torch.argmax(logits_nat.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        total_correct_adv += torch.argmax(logits_adv.data, dim=1).eq(targets.data).cpu().sum().item()\n",
    "        \n",
    "    print('Validation | acc (nat): %.4f | acc (rob): %.4f |' % (total_correct_nat / len(dataloader.dataset),\n",
    "                                                                total_correct_adv / len(dataloader.dataset)))\n",
    "    return (total_correct_nat / len(dataloader.dataset)), (total_correct_adv / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208f000e-cb35-4ad5-93d5-fe47f788b266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/1838] | loss: 2.3024 (avg: 0.0013) | acc: 0.0938 (avg: 0.0938) |\n",
      "Epoch 0 [100/1838] | loss: 1.8402 (avg: 0.1156) | acc: 0.3125 (avg: 0.2203) |\n",
      "Epoch 0 [200/1838] | loss: 1.0452 (avg: 0.1891) | acc: 0.5938 (avg: 0.3730) |\n",
      "Epoch 0 [300/1838] | loss: 0.4394 (avg: 0.2338) | acc: 0.8750 (avg: 0.4949) |\n",
      "Epoch 0 [400/1838] | loss: 0.3966 (avg: 0.2665) | acc: 0.9375 (avg: 0.5742) |\n",
      "Epoch 0 [500/1838] | loss: 0.4528 (avg: 0.2949) | acc: 0.8438 (avg: 0.6266) |\n",
      "Epoch 0 [600/1838] | loss: 0.4231 (avg: 0.3181) | acc: 0.8750 (avg: 0.6680) |\n",
      "Epoch 0 [700/1838] | loss: 0.2976 (avg: 0.3379) | acc: 0.9375 (avg: 0.6995) |\n",
      "Epoch 0 [800/1838] | loss: 0.4086 (avg: 0.3577) | acc: 0.8438 (avg: 0.7232) |\n",
      "Epoch 0 [900/1838] | loss: 0.2564 (avg: 0.3750) | acc: 0.9375 (avg: 0.7429) |\n",
      "Epoch 0 [1000/1838] | loss: 0.3460 (avg: 0.3917) | acc: 0.8750 (avg: 0.7596) |\n",
      "Epoch 0 [1100/1838] | loss: 0.9061 (avg: 0.4063) | acc: 0.8438 (avg: 0.7738) |\n",
      "Epoch 0 [1200/1838] | loss: 0.1152 (avg: 0.4201) | acc: 0.9688 (avg: 0.7855) |\n",
      "Epoch 0 [1300/1838] | loss: 0.3165 (avg: 0.4361) | acc: 0.8750 (avg: 0.7954) |\n",
      "Epoch 0 [1400/1838] | loss: 0.4245 (avg: 0.4510) | acc: 0.9062 (avg: 0.8042) |\n",
      "Epoch 0 [1500/1838] | loss: 0.3401 (avg: 0.4653) | acc: 0.9062 (avg: 0.8124) |\n",
      "Epoch 0 [1600/1838] | loss: 0.3318 (avg: 0.4781) | acc: 0.8750 (avg: 0.8196) |\n",
      "Epoch 0 [1700/1838] | loss: 0.3654 (avg: 0.4905) | acc: 0.9062 (avg: 0.8259) |\n",
      "Epoch 0 [1800/1838] | loss: 0.1263 (avg: 0.5027) | acc: 0.9375 (avg: 0.8317) |\n",
      "Validation | acc (nat): 0.9458 | acc (rob): 0.0900 |\n",
      "Epoch 1 [0/1838] | loss: 0.6002 (avg: 0.0003) | acc: 0.8125 (avg: 0.8125) |\n",
      "Epoch 1 [100/1838] | loss: 0.2987 (avg: 0.0122) | acc: 0.9688 (avg: 0.9347) |\n",
      "Epoch 1 [200/1838] | loss: 0.4924 (avg: 0.0230) | acc: 0.8750 (avg: 0.9391) |\n",
      "Epoch 1 [300/1838] | loss: 0.2698 (avg: 0.0344) | acc: 0.9062 (avg: 0.9382) |\n",
      "Epoch 1 [400/1838] | loss: 0.1860 (avg: 0.0445) | acc: 0.9688 (avg: 0.9392) |\n",
      "Epoch 1 [500/1838] | loss: 0.1456 (avg: 0.0545) | acc: 0.9375 (avg: 0.9402) |\n",
      "Epoch 1 [600/1838] | loss: 0.4698 (avg: 0.0641) | acc: 0.9062 (avg: 0.9412) |\n",
      "Epoch 1 [700/1838] | loss: 0.1851 (avg: 0.0751) | acc: 0.9375 (avg: 0.9410) |\n",
      "Epoch 1 [800/1838] | loss: 0.6005 (avg: 0.0858) | acc: 0.8125 (avg: 0.9411) |\n",
      "Epoch 1 [900/1838] | loss: 0.0285 (avg: 0.0958) | acc: 1.0000 (avg: 0.9416) |\n",
      "Epoch 1 [1000/1838] | loss: 0.0729 (avg: 0.1052) | acc: 0.9688 (avg: 0.9422) |\n",
      "Epoch 1 [1100/1838] | loss: 0.0721 (avg: 0.1153) | acc: 0.9688 (avg: 0.9424) |\n",
      "Epoch 1 [1200/1838] | loss: 0.1850 (avg: 0.1245) | acc: 0.9375 (avg: 0.9430) |\n",
      "Epoch 1 [1300/1838] | loss: 0.0676 (avg: 0.1330) | acc: 0.9688 (avg: 0.9436) |\n",
      "Epoch 1 [1400/1838] | loss: 0.0955 (avg: 0.1413) | acc: 0.9688 (avg: 0.9443) |\n",
      "Epoch 1 [1500/1838] | loss: 0.1464 (avg: 0.1495) | acc: 0.9375 (avg: 0.9449) |\n",
      "Epoch 1 [1600/1838] | loss: 0.1022 (avg: 0.1594) | acc: 0.9688 (avg: 0.9449) |\n",
      "Epoch 1 [1700/1838] | loss: 0.0325 (avg: 0.1685) | acc: 1.0000 (avg: 0.9452) |\n",
      "Epoch 1 [1800/1838] | loss: 0.1067 (avg: 0.1767) | acc: 0.9688 (avg: 0.9455) |\n",
      "Validation | acc (nat): 0.9475 | acc (rob): 0.0900 |\n",
      "Epoch 2 [0/1838] | loss: 0.3031 (avg: 0.0002) | acc: 0.8750 (avg: 0.8750) |\n",
      "Epoch 2 [100/1838] | loss: 0.2938 (avg: 0.0094) | acc: 0.9062 (avg: 0.9446) |\n",
      "Epoch 2 [200/1838] | loss: 0.0359 (avg: 0.0177) | acc: 1.0000 (avg: 0.9481) |\n",
      "Epoch 2 [300/1838] | loss: 0.0750 (avg: 0.0266) | acc: 1.0000 (avg: 0.9484) |\n",
      "Epoch 2 [400/1838] | loss: 0.0581 (avg: 0.0341) | acc: 1.0000 (avg: 0.9515) |\n",
      "Epoch 2 [500/1838] | loss: 0.2641 (avg: 0.0417) | acc: 0.9062 (avg: 0.9527) |\n",
      "Epoch 2 [600/1838] | loss: 0.2133 (avg: 0.0507) | acc: 0.8750 (avg: 0.9521) |\n",
      "Epoch 2 [700/1838] | loss: 0.0656 (avg: 0.0585) | acc: 1.0000 (avg: 0.9527) |\n",
      "Epoch 2 [800/1838] | loss: 0.0407 (avg: 0.0656) | acc: 1.0000 (avg: 0.9535) |\n",
      "Epoch 2 [900/1838] | loss: 0.1383 (avg: 0.0739) | acc: 0.9688 (avg: 0.9536) |\n",
      "Epoch 2 [1000/1838] | loss: 0.2519 (avg: 0.0813) | acc: 0.8750 (avg: 0.9542) |\n",
      "Epoch 2 [1100/1838] | loss: 0.1316 (avg: 0.0892) | acc: 0.9375 (avg: 0.9543) |\n",
      "Epoch 2 [1200/1838] | loss: 0.0641 (avg: 0.0972) | acc: 0.9688 (avg: 0.9549) |\n",
      "Epoch 2 [1300/1838] | loss: 0.1215 (avg: 0.1048) | acc: 0.9688 (avg: 0.9551) |\n",
      "Epoch 2 [1400/1838] | loss: 0.2468 (avg: 0.1131) | acc: 0.9375 (avg: 0.9550) |\n",
      "Epoch 2 [1500/1838] | loss: 0.1639 (avg: 0.1203) | acc: 0.9375 (avg: 0.9554) |\n",
      "Epoch 2 [1600/1838] | loss: 0.1090 (avg: 0.1281) | acc: 0.9688 (avg: 0.9558) |\n",
      "Epoch 2 [1700/1838] | loss: 0.0191 (avg: 0.1354) | acc: 1.0000 (avg: 0.9561) |\n",
      "Epoch 2 [1800/1838] | loss: 0.2120 (avg: 0.1419) | acc: 0.9062 (avg: 0.9565) |\n",
      "Validation | acc (nat): 0.9650 | acc (rob): 0.1183 |\n",
      "Epoch 3 [0/1838] | loss: 0.2484 (avg: 0.0001) | acc: 0.9062 (avg: 0.9062) |\n",
      "Epoch 3 [100/1838] | loss: 0.0433 (avg: 0.0065) | acc: 1.0000 (avg: 0.9607) |\n",
      "Epoch 3 [200/1838] | loss: 0.1848 (avg: 0.0138) | acc: 0.8750 (avg: 0.9610) |\n",
      "Epoch 3 [300/1838] | loss: 0.1050 (avg: 0.0206) | acc: 0.9375 (avg: 0.9616) |\n",
      "Epoch 3 [400/1838] | loss: 0.1194 (avg: 0.0271) | acc: 0.9688 (avg: 0.9610) |\n",
      "Epoch 3 [500/1838] | loss: 0.0099 (avg: 0.0336) | acc: 1.0000 (avg: 0.9611) |\n",
      "Epoch 3 [600/1838] | loss: 0.0373 (avg: 0.0398) | acc: 1.0000 (avg: 0.9616) |\n",
      "Epoch 3 [700/1838] | loss: 0.1836 (avg: 0.0468) | acc: 0.9688 (avg: 0.9613) |\n",
      "Epoch 3 [800/1838] | loss: 0.0189 (avg: 0.0528) | acc: 1.0000 (avg: 0.9618) |\n",
      "Epoch 3 [900/1838] | loss: 0.1477 (avg: 0.0593) | acc: 0.9688 (avg: 0.9621) |\n",
      "Epoch 3 [1000/1838] | loss: 0.1233 (avg: 0.0651) | acc: 0.9688 (avg: 0.9625) |\n",
      "Epoch 3 [1100/1838] | loss: 0.2353 (avg: 0.0715) | acc: 0.9375 (avg: 0.9627) |\n",
      "Epoch 3 [1200/1838] | loss: 0.2103 (avg: 0.0776) | acc: 0.9375 (avg: 0.9635) |\n",
      "Epoch 3 [1300/1838] | loss: 0.0112 (avg: 0.0832) | acc: 1.0000 (avg: 0.9640) |\n",
      "Epoch 3 [1400/1838] | loss: 0.0685 (avg: 0.0896) | acc: 0.9688 (avg: 0.9641) |\n",
      "Epoch 3 [1500/1838] | loss: 0.0095 (avg: 0.0959) | acc: 1.0000 (avg: 0.9644) |\n",
      "Epoch 3 [1600/1838] | loss: 0.1117 (avg: 0.1030) | acc: 0.9688 (avg: 0.9641) |\n",
      "Epoch 3 [1700/1838] | loss: 0.2889 (avg: 0.1102) | acc: 0.9375 (avg: 0.9638) |\n",
      "Epoch 3 [1800/1838] | loss: 0.0667 (avg: 0.1162) | acc: 0.9688 (avg: 0.9639) |\n",
      "Validation | acc (nat): 0.9658 | acc (rob): 0.0900 |\n",
      "Epoch 4 [0/1838] | loss: 0.1214 (avg: 0.0001) | acc: 0.9375 (avg: 0.9375) |\n",
      "Epoch 4 [100/1838] | loss: 0.0593 (avg: 0.0059) | acc: 0.9688 (avg: 0.9675) |\n",
      "Epoch 4 [200/1838] | loss: 0.0489 (avg: 0.0117) | acc: 1.0000 (avg: 0.9688) |\n",
      "Epoch 4 [300/1838] | loss: 0.0781 (avg: 0.0176) | acc: 0.9688 (avg: 0.9676) |\n",
      "Epoch 4 [400/1838] | loss: 0.1789 (avg: 0.0237) | acc: 0.9688 (avg: 0.9678) |\n",
      "Epoch 4 [500/1838] | loss: 0.0249 (avg: 0.0285) | acc: 1.0000 (avg: 0.9689) |\n",
      "Epoch 4 [600/1838] | loss: 0.0630 (avg: 0.0347) | acc: 0.9688 (avg: 0.9685) |\n",
      "Epoch 4 [700/1838] | loss: 0.0451 (avg: 0.0396) | acc: 0.9688 (avg: 0.9689) |\n",
      "Epoch 4 [800/1838] | loss: 0.1102 (avg: 0.0452) | acc: 0.9688 (avg: 0.9693) |\n",
      "Epoch 4 [900/1838] | loss: 0.0771 (avg: 0.0502) | acc: 0.9375 (avg: 0.9694) |\n",
      "Epoch 4 [1000/1838] | loss: 0.2964 (avg: 0.0566) | acc: 0.9375 (avg: 0.9688) |\n",
      "Epoch 4 [1100/1838] | loss: 0.0525 (avg: 0.0634) | acc: 0.9688 (avg: 0.9683) |\n",
      "Epoch 4 [1200/1838] | loss: 0.0820 (avg: 0.0688) | acc: 0.9688 (avg: 0.9683) |\n",
      "Epoch 4 [1300/1838] | loss: 0.0756 (avg: 0.0737) | acc: 0.9688 (avg: 0.9684) |\n",
      "Epoch 4 [1400/1838] | loss: 0.2419 (avg: 0.0799) | acc: 0.9375 (avg: 0.9684) |\n",
      "Epoch 4 [1500/1838] | loss: 0.0249 (avg: 0.0852) | acc: 1.0000 (avg: 0.9684) |\n",
      "Epoch 4 [1600/1838] | loss: 0.1547 (avg: 0.0903) | acc: 0.9375 (avg: 0.9685) |\n",
      "Epoch 4 [1700/1838] | loss: 0.0510 (avg: 0.0961) | acc: 0.9688 (avg: 0.9686) |\n",
      "Epoch 4 [1800/1838] | loss: 0.0305 (avg: 0.1018) | acc: 1.0000 (avg: 0.9686) |\n",
      "Validation | acc (nat): 0.9733 | acc (rob): 0.0900 |\n",
      "Epoch 5 [0/1838] | loss: 0.0276 (avg: 0.0000) | acc: 1.0000 (avg: 1.0000) |\n",
      "Epoch 5 [100/1838] | loss: 0.0363 (avg: 0.0059) | acc: 1.0000 (avg: 0.9691) |\n",
      "Epoch 5 [200/1838] | loss: 0.0065 (avg: 0.0106) | acc: 1.0000 (avg: 0.9728) |\n",
      "Epoch 5 [300/1838] | loss: 0.0567 (avg: 0.0163) | acc: 0.9688 (avg: 0.9712) |\n",
      "Epoch 5 [400/1838] | loss: 0.0094 (avg: 0.0212) | acc: 1.0000 (avg: 0.9716) |\n",
      "Epoch 5 [500/1838] | loss: 0.0245 (avg: 0.0267) | acc: 1.0000 (avg: 0.9718) |\n",
      "Epoch 5 [600/1838] | loss: 0.0518 (avg: 0.0322) | acc: 0.9688 (avg: 0.9713) |\n",
      "Epoch 5 [700/1838] | loss: 0.0150 (avg: 0.0381) | acc: 1.0000 (avg: 0.9713) |\n",
      "Epoch 5 [800/1838] | loss: 0.0341 (avg: 0.0432) | acc: 1.0000 (avg: 0.9717) |\n",
      "Epoch 5 [900/1838] | loss: 0.4450 (avg: 0.0486) | acc: 0.9062 (avg: 0.9716) |\n",
      "Epoch 5 [1000/1838] | loss: 0.0205 (avg: 0.0537) | acc: 1.0000 (avg: 0.9717) |\n",
      "Epoch 5 [1100/1838] | loss: 0.0624 (avg: 0.0590) | acc: 0.9688 (avg: 0.9717) |\n",
      "Epoch 5 [1200/1838] | loss: 0.0284 (avg: 0.0635) | acc: 1.0000 (avg: 0.9717) |\n",
      "Epoch 5 [1300/1838] | loss: 0.0344 (avg: 0.0683) | acc: 0.9688 (avg: 0.9718) |\n",
      "Epoch 5 [1400/1838] | loss: 0.0892 (avg: 0.0732) | acc: 0.9688 (avg: 0.9719) |\n",
      "Epoch 5 [1500/1838] | loss: 0.0812 (avg: 0.0780) | acc: 0.9688 (avg: 0.9718) |\n",
      "Epoch 5 [1600/1838] | loss: 0.0493 (avg: 0.0828) | acc: 0.9688 (avg: 0.9717) |\n",
      "Epoch 5 [1700/1838] | loss: 0.0597 (avg: 0.0871) | acc: 0.9688 (avg: 0.9719) |\n",
      "Epoch 5 [1800/1838] | loss: 0.0525 (avg: 0.0920) | acc: 0.9688 (avg: 0.9719) |\n",
      "Validation | acc (nat): 0.9717 | acc (rob): 0.0900 |\n",
      "Epoch 6 [0/1838] | loss: 0.0170 (avg: 0.0000) | acc: 1.0000 (avg: 1.0000) |\n",
      "Epoch 6 [100/1838] | loss: 0.0556 (avg: 0.0041) | acc: 1.0000 (avg: 0.9777) |\n",
      "Epoch 6 [200/1838] | loss: 0.0385 (avg: 0.0087) | acc: 1.0000 (avg: 0.9756) |\n",
      "Epoch 6 [300/1838] | loss: 0.0650 (avg: 0.0142) | acc: 0.9688 (avg: 0.9735) |\n",
      "Epoch 6 [400/1838] | loss: 0.0700 (avg: 0.0188) | acc: 0.9688 (avg: 0.9737) |\n",
      "Epoch 6 [500/1838] | loss: 0.0276 (avg: 0.0230) | acc: 1.0000 (avg: 0.9744) |\n",
      "Epoch 6 [600/1838] | loss: 0.0368 (avg: 0.0282) | acc: 1.0000 (avg: 0.9739) |\n",
      "Epoch 6 [700/1838] | loss: 0.3016 (avg: 0.0322) | acc: 0.9688 (avg: 0.9742) |\n",
      "Epoch 6 [800/1838] | loss: 0.0042 (avg: 0.0371) | acc: 1.0000 (avg: 0.9738) |\n",
      "Epoch 6 [900/1838] | loss: 0.0434 (avg: 0.0415) | acc: 1.0000 (avg: 0.9743) |\n",
      "Epoch 6 [1000/1838] | loss: 0.0049 (avg: 0.0459) | acc: 1.0000 (avg: 0.9743) |\n",
      "Epoch 6 [1100/1838] | loss: 0.0344 (avg: 0.0498) | acc: 0.9688 (avg: 0.9747) |\n",
      "Epoch 6 [1200/1838] | loss: 0.1163 (avg: 0.0551) | acc: 0.9375 (avg: 0.9746) |\n",
      "Epoch 6 [1300/1838] | loss: 0.0462 (avg: 0.0588) | acc: 1.0000 (avg: 0.9750) |\n",
      "Epoch 6 [1400/1838] | loss: 0.0510 (avg: 0.0633) | acc: 0.9688 (avg: 0.9751) |\n",
      "Epoch 6 [1500/1838] | loss: 0.0483 (avg: 0.0685) | acc: 0.9688 (avg: 0.9749) |\n",
      "Epoch 6 [1600/1838] | loss: 0.0056 (avg: 0.0732) | acc: 1.0000 (avg: 0.9748) |\n",
      "Epoch 6 [1700/1838] | loss: 0.0083 (avg: 0.0774) | acc: 1.0000 (avg: 0.9747) |\n",
      "Epoch 6 [1800/1838] | loss: 0.0191 (avg: 0.0821) | acc: 1.0000 (avg: 0.9746) |\n",
      "Validation | acc (nat): 0.9775 | acc (rob): 0.0900 |\n",
      "Epoch 7 [0/1838] | loss: 0.1467 (avg: 0.0001) | acc: 0.9688 (avg: 0.9688) |\n",
      "Epoch 7 [100/1838] | loss: 0.0640 (avg: 0.0048) | acc: 0.9688 (avg: 0.9756) |\n",
      "Epoch 7 [200/1838] | loss: 0.0167 (avg: 0.0090) | acc: 1.0000 (avg: 0.9757) |\n",
      "Epoch 7 [300/1838] | loss: 0.1542 (avg: 0.0131) | acc: 0.9688 (avg: 0.9757) |\n",
      "Epoch 7 [400/1838] | loss: 0.0523 (avg: 0.0162) | acc: 0.9688 (avg: 0.9775) |\n",
      "Epoch 7 [500/1838] | loss: 0.2561 (avg: 0.0200) | acc: 0.9062 (avg: 0.9780) |\n",
      "Epoch 7 [600/1838] | loss: 0.1824 (avg: 0.0245) | acc: 0.9062 (avg: 0.9776) |\n",
      "Epoch 7 [700/1838] | loss: 0.0529 (avg: 0.0278) | acc: 0.9688 (avg: 0.9785) |\n",
      "Epoch 7 [800/1838] | loss: 0.0074 (avg: 0.0333) | acc: 1.0000 (avg: 0.9782) |\n",
      "Epoch 7 [900/1838] | loss: 0.0251 (avg: 0.0380) | acc: 1.0000 (avg: 0.9776) |\n",
      "Epoch 7 [1000/1838] | loss: 0.0665 (avg: 0.0421) | acc: 1.0000 (avg: 0.9776) |\n",
      "Epoch 7 [1100/1838] | loss: 0.1654 (avg: 0.0466) | acc: 0.9688 (avg: 0.9774) |\n",
      "Epoch 7 [1200/1838] | loss: 0.0045 (avg: 0.0512) | acc: 1.0000 (avg: 0.9772) |\n",
      "Epoch 7 [1300/1838] | loss: 0.1165 (avg: 0.0551) | acc: 0.9688 (avg: 0.9772) |\n",
      "Epoch 7 [1400/1838] | loss: 0.0389 (avg: 0.0587) | acc: 0.9688 (avg: 0.9774) |\n",
      "Epoch 7 [1500/1838] | loss: 0.0123 (avg: 0.0633) | acc: 1.0000 (avg: 0.9772) |\n",
      "Epoch 7 [1600/1838] | loss: 0.0162 (avg: 0.0671) | acc: 1.0000 (avg: 0.9773) |\n",
      "Epoch 7 [1700/1838] | loss: 0.0766 (avg: 0.0716) | acc: 0.9375 (avg: 0.9771) |\n",
      "Epoch 7 [1800/1838] | loss: 0.0070 (avg: 0.0762) | acc: 1.0000 (avg: 0.9769) |\n",
      "Validation | acc (nat): 0.9758 | acc (rob): 0.0900 |\n",
      "Epoch 8 [0/1838] | loss: 0.0590 (avg: 0.0000) | acc: 0.9688 (avg: 0.9688) |\n",
      "Epoch 8 [100/1838] | loss: 0.0287 (avg: 0.0035) | acc: 1.0000 (avg: 0.9793) |\n",
      "Epoch 8 [200/1838] | loss: 0.0093 (avg: 0.0070) | acc: 1.0000 (avg: 0.9792) |\n",
      "Epoch 8 [300/1838] | loss: 0.0934 (avg: 0.0116) | acc: 0.9688 (avg: 0.9773) |\n",
      "Epoch 8 [400/1838] | loss: 0.0202 (avg: 0.0154) | acc: 1.0000 (avg: 0.9778) |\n",
      "Epoch 8 [500/1838] | loss: 0.0239 (avg: 0.0193) | acc: 1.0000 (avg: 0.9784) |\n",
      "Epoch 8 [600/1838] | loss: 0.0072 (avg: 0.0229) | acc: 1.0000 (avg: 0.9785) |\n",
      "Epoch 8 [700/1838] | loss: 0.0218 (avg: 0.0275) | acc: 1.0000 (avg: 0.9776) |\n",
      "Epoch 8 [800/1838] | loss: 0.0049 (avg: 0.0317) | acc: 1.0000 (avg: 0.9778) |\n",
      "Epoch 8 [900/1838] | loss: 0.0255 (avg: 0.0362) | acc: 1.0000 (avg: 0.9775) |\n",
      "Epoch 8 [1000/1838] | loss: 0.0133 (avg: 0.0397) | acc: 1.0000 (avg: 0.9776) |\n",
      "Epoch 8 [1100/1838] | loss: 0.0974 (avg: 0.0438) | acc: 0.9688 (avg: 0.9777) |\n",
      "Epoch 8 [1200/1838] | loss: 0.2054 (avg: 0.0476) | acc: 0.9688 (avg: 0.9782) |\n",
      "Epoch 8 [1300/1838] | loss: 0.0015 (avg: 0.0510) | acc: 1.0000 (avg: 0.9784) |\n",
      "Epoch 8 [1400/1838] | loss: 0.0329 (avg: 0.0551) | acc: 0.9688 (avg: 0.9782) |\n",
      "Epoch 8 [1500/1838] | loss: 0.0362 (avg: 0.0588) | acc: 1.0000 (avg: 0.9784) |\n",
      "Epoch 8 [1600/1838] | loss: 0.0363 (avg: 0.0629) | acc: 0.9688 (avg: 0.9782) |\n",
      "Epoch 8 [1700/1838] | loss: 0.2064 (avg: 0.0668) | acc: 0.9688 (avg: 0.9783) |\n",
      "Epoch 8 [1800/1838] | loss: 0.0105 (avg: 0.0706) | acc: 1.0000 (avg: 0.9784) |\n",
      "Validation | acc (nat): 0.9783 | acc (rob): 0.0900 |\n",
      "Epoch 9 [0/1838] | loss: 0.0453 (avg: 0.0000) | acc: 0.9688 (avg: 0.9688) |\n",
      "Epoch 9 [100/1838] | loss: 0.0989 (avg: 0.0030) | acc: 0.9688 (avg: 0.9830) |\n",
      "Epoch 9 [200/1838] | loss: 0.0169 (avg: 0.0065) | acc: 1.0000 (avg: 0.9823) |\n",
      "Epoch 9 [300/1838] | loss: 0.0936 (avg: 0.0102) | acc: 0.9688 (avg: 0.9820) |\n",
      "Epoch 9 [400/1838] | loss: 0.0305 (avg: 0.0143) | acc: 1.0000 (avg: 0.9804) |\n",
      "Epoch 9 [500/1838] | loss: 0.0186 (avg: 0.0184) | acc: 1.0000 (avg: 0.9799) |\n",
      "Epoch 9 [600/1838] | loss: 0.0265 (avg: 0.0212) | acc: 1.0000 (avg: 0.9802) |\n",
      "Epoch 9 [700/1838] | loss: 0.0822 (avg: 0.0246) | acc: 0.9688 (avg: 0.9801) |\n",
      "Epoch 9 [800/1838] | loss: 0.0055 (avg: 0.0282) | acc: 1.0000 (avg: 0.9799) |\n",
      "Epoch 9 [900/1838] | loss: 0.0432 (avg: 0.0324) | acc: 0.9688 (avg: 0.9795) |\n",
      "Epoch 9 [1000/1838] | loss: 0.0215 (avg: 0.0372) | acc: 1.0000 (avg: 0.9792) |\n",
      "Epoch 9 [1100/1838] | loss: 0.1199 (avg: 0.0409) | acc: 0.9375 (avg: 0.9790) |\n",
      "Epoch 9 [1200/1838] | loss: 0.0403 (avg: 0.0436) | acc: 0.9688 (avg: 0.9793) |\n",
      "Epoch 9 [1300/1838] | loss: 0.2068 (avg: 0.0473) | acc: 0.9062 (avg: 0.9792) |\n",
      "Epoch 9 [1400/1838] | loss: 0.0093 (avg: 0.0507) | acc: 1.0000 (avg: 0.9795) |\n",
      "Epoch 9 [1500/1838] | loss: 0.0326 (avg: 0.0544) | acc: 1.0000 (avg: 0.9795) |\n",
      "Epoch 9 [1600/1838] | loss: 0.0529 (avg: 0.0575) | acc: 0.9688 (avg: 0.9797) |\n",
      "Epoch 9 [1700/1838] | loss: 0.1202 (avg: 0.0617) | acc: 0.9375 (avg: 0.9796) |\n",
      "Epoch 9 [1800/1838] | loss: 0.0518 (avg: 0.0654) | acc: 0.9688 (avg: 0.9795) |\n",
      "Validation | acc (nat): 0.9792 | acc (rob): 0.1183 |\n",
      "Epoch 10 [0/1838] | loss: 0.0100 (avg: 0.0000) | acc: 1.0000 (avg: 1.0000) |\n",
      "Epoch 10 [100/1838] | loss: 0.1113 (avg: 0.0035) | acc: 0.9688 (avg: 0.9768) |\n",
      "Epoch 10 [200/1838] | loss: 0.0286 (avg: 0.0064) | acc: 0.9688 (avg: 0.9795) |\n",
      "Epoch 10 [300/1838] | loss: 0.0170 (avg: 0.0101) | acc: 1.0000 (avg: 0.9791) |\n",
      "Epoch 10 [400/1838] | loss: 0.0680 (avg: 0.0130) | acc: 0.9688 (avg: 0.9804) |\n",
      "Epoch 10 [500/1838] | loss: 0.0222 (avg: 0.0162) | acc: 1.0000 (avg: 0.9807) |\n",
      "Epoch 10 [600/1838] | loss: 0.0063 (avg: 0.0194) | acc: 1.0000 (avg: 0.9809) |\n",
      "Epoch 10 [700/1838] | loss: 0.0049 (avg: 0.0226) | acc: 1.0000 (avg: 0.9810) |\n",
      "Epoch 10 [800/1838] | loss: 0.0300 (avg: 0.0264) | acc: 0.9688 (avg: 0.9808) |\n",
      "Epoch 10 [900/1838] | loss: 0.0145 (avg: 0.0300) | acc: 1.0000 (avg: 0.9808) |\n",
      "Epoch 10 [1000/1838] | loss: 0.0296 (avg: 0.0338) | acc: 1.0000 (avg: 0.9804) |\n",
      "Epoch 10 [1100/1838] | loss: 0.0499 (avg: 0.0369) | acc: 0.9688 (avg: 0.9806) |\n",
      "Epoch 10 [1200/1838] | loss: 0.0261 (avg: 0.0401) | acc: 1.0000 (avg: 0.9808) |\n",
      "Epoch 10 [1300/1838] | loss: 0.0376 (avg: 0.0433) | acc: 1.0000 (avg: 0.9809) |\n",
      "Epoch 10 [1400/1838] | loss: 0.0137 (avg: 0.0464) | acc: 1.0000 (avg: 0.9810) |\n",
      "Epoch 10 [1500/1838] | loss: 0.3025 (avg: 0.0504) | acc: 0.9375 (avg: 0.9808) |\n",
      "Epoch 10 [1600/1838] | loss: 0.0245 (avg: 0.0541) | acc: 1.0000 (avg: 0.9808) |\n",
      "Epoch 10 [1700/1838] | loss: 0.0154 (avg: 0.0570) | acc: 1.0000 (avg: 0.9811) |\n",
      "Epoch 10 [1800/1838] | loss: 0.0791 (avg: 0.0615) | acc: 0.9688 (avg: 0.9809) |\n",
      "Validation | acc (nat): 0.9883 | acc (rob): 0.0900 |\n",
      "Epoch 11 [0/1838] | loss: 0.1490 (avg: 0.0001) | acc: 0.9688 (avg: 0.9688) |\n",
      "Epoch 11 [100/1838] | loss: 0.0624 (avg: 0.0028) | acc: 0.9688 (avg: 0.9817) |\n",
      "Epoch 11 [200/1838] | loss: 0.0006 (avg: 0.0060) | acc: 1.0000 (avg: 0.9820) |\n",
      "Epoch 11 [300/1838] | loss: 0.1784 (avg: 0.0099) | acc: 0.9375 (avg: 0.9809) |\n",
      "Epoch 11 [400/1838] | loss: 0.0406 (avg: 0.0136) | acc: 0.9688 (avg: 0.9803) |\n",
      "Epoch 11 [500/1838] | loss: 0.0421 (avg: 0.0175) | acc: 0.9688 (avg: 0.9794) |\n",
      "Epoch 11 [600/1838] | loss: 0.0750 (avg: 0.0203) | acc: 0.9375 (avg: 0.9801) |\n",
      "Epoch 11 [700/1838] | loss: 0.0134 (avg: 0.0232) | acc: 1.0000 (avg: 0.9806) |\n",
      "Epoch 11 [800/1838] | loss: 0.0038 (avg: 0.0259) | acc: 1.0000 (avg: 0.9808) |\n",
      "Epoch 11 [900/1838] | loss: 0.0241 (avg: 0.0295) | acc: 1.0000 (avg: 0.9807) |\n",
      "Epoch 11 [1000/1838] | loss: 0.0490 (avg: 0.0338) | acc: 0.9688 (avg: 0.9803) |\n",
      "Epoch 11 [1100/1838] | loss: 0.0029 (avg: 0.0369) | acc: 1.0000 (avg: 0.9804) |\n",
      "Epoch 11 [1200/1838] | loss: 0.0636 (avg: 0.0399) | acc: 0.9688 (avg: 0.9806) |\n",
      "Epoch 11 [1300/1838] | loss: 0.0835 (avg: 0.0428) | acc: 0.9688 (avg: 0.9808) |\n",
      "Epoch 11 [1400/1838] | loss: 0.0074 (avg: 0.0466) | acc: 1.0000 (avg: 0.9808) |\n",
      "Epoch 11 [1500/1838] | loss: 0.0636 (avg: 0.0496) | acc: 0.9688 (avg: 0.9809) |\n",
      "Epoch 11 [1600/1838] | loss: 0.0104 (avg: 0.0531) | acc: 1.0000 (avg: 0.9809) |\n",
      "Epoch 11 [1700/1838] | loss: 0.0233 (avg: 0.0558) | acc: 1.0000 (avg: 0.9811) |\n",
      "Epoch 11 [1800/1838] | loss: 0.0975 (avg: 0.0581) | acc: 0.9688 (avg: 0.9814) |\n",
      "Validation | acc (nat): 0.9833 | acc (rob): 0.1183 |\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "train_dataset, _ = get_dataloader(dataset, batch_size, image_size=28)\n",
    "num_samples = len(train_dataset)\n",
    "num_samples_for_train = int(num_samples * 0.98)\n",
    "num_samples_for_valid = num_samples - num_samples_for_train\n",
    "train_set, valid_set = random_split(train_dataset, [num_samples_for_train, num_samples_for_valid])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "model = nn.DataParallel(Kernel_trick(num_classes).cuda())\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_acc_nat, best_acc_rob = 0, 0\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    training(epoch, model, train_dataloader, optimizer, num_classes)\n",
    "    test_acc_nat, test_acc_rob = evaluation(epoch, model, valid_dataloader, alpha, epsilon, num_repeats)\n",
    "        \n",
    "    is_best = best_acc_nat < test_acc_nat and best_acc_rob < test_acc_rob\n",
    "    best_acc_nat = max(best_acc_nat, test_acc_nat)\n",
    "    best_acc_rob = max(best_acc_rob, test_acc_rob)\n",
    "    save_checkpoint = {'state_dict': model.state_dict(),\n",
    "                       'best_acc_nat': best_acc_nat,\n",
    "                       'best_acc_rob': best_acc_rob,\n",
    "                       'optimizer': optimizer.state_dict(),\n",
    "                       'model_type': model_type,\n",
    "                       'dataset': dataset}\n",
    "    torch.save(save_checkpoint, os.path.join(checkpoint, 'model'))\n",
    "    if is_best:\n",
    "        torch.save(save_checkpoint, os.path.join(checkpoint, 'best_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67273057-ddb6-4294-b7ae-3aa932ab0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2ddb7-2371-419a-8bdc-86a3775a813d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
